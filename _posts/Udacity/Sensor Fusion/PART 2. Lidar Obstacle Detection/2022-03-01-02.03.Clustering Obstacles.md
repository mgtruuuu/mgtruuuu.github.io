---
title : "02.03 — Clustering Obstacles"
category :
    - Sensor Fusion
tag : 
    - C++
    - https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313
toc: true  
toc_sticky: true 
use_math : true
---

recent check: 03/14/2022



## 1. What is Clustering?

{% include video id="w7EmEiKPMeQ" provider="youtube" %}



## 2. Clustering Obstacles

{% include video id="QCb00ohqITw" provider="youtube" %}

You have a way to segment points and recognize which ones represent obstacles for your car. It would be great to break up and group those obstacle points, especially if you want to do multiple object tracking with cars, pedestrians, and bicyclists, for instance. One way to do that grouping and cluster point cloud data is called **euclidean clustering**.


### Euclidean Clustering

**The idea is you associate groups of points by how close together they are.** To do a nearest neighbor search efficiently, you use a **KD-Tree** data structure which, on average, speeds up your look up time from `O(n)` to `O(log(n))`. This is because the tree allows you to better break up your search space. By grouping points into regions in a KD-Tree, you can avoid calculating distance for possibly thousands of points just because you know they are not even considered in a close enough region.

In this lesson, you will begin by seeing how to do Euclidean clustering using built-in PCL functions. Next, you will write your own clustering algorithm using a KD-Tree. (Your implementation will be used in your project submission, so be sure to complete the implementation in the exercises that follow!)



## 3. Euclidean Clustering with PCL

{% include video id="LDHO-U4G0k0" provider="youtube" %}

Inside `pointProcessor`, the `Clustering` function is located right under the `SegmentPlane` function that you previously were working on.

PCL provides some documentation for using its built in [euclidean clustering functions](https://pointclouds.org/documentation/tutorials/cluster_extraction.html). In particular check out lines 71-82.


### Euclidean Clustering Arguments

The euclidean clustering object `ec` takes in a *distance tolerance*. Any points within that distance will be grouped together. It also has *min and max arguments for the number of points to represent as clusters*. The idea is: 
- *if a cluster is really small*, it’s probably just noise and we are not concerned with it.
Also a max number of points allows us to better break up very large clusters. 
- *If a cluster is very large* it might just be that many other clusters are overlapping, and a max tolerance can help us better resolve the object detections. 
The last argument to the euclidean cluster object is the *Kd-Tree*. The tree is created and built using the input cloud points, which in this case are going to be the obstacle cloud points.

Back in `environment.cpp` let's see how to render the different clusters.

```c++
std::vector<pcl::PointCloud<pcl::PointXYZ>::Ptr> cloudClusters{ 
    pointProcessor->Clustering(segmentCloud.first, 1.0, 3, 30)
};

int clusterId{ 0 };
std::vector<Color> colors{ Color{ 1, 0, 0 }, Color{ 0, 1, 0 }, Color{ 0, 0, 1 } };

for (pcl::PointCloud<pcl::PointXYZ>::Ptr cluster : cloudClusters) {
    std::cout << "cluster size ";
    pointProcessor->numPoints(cluster);
    renderPointCloud(viewer, cluster, "obstCloud" + std::to_string(clusterId), colors[clusterId]);
    ++clusterId;
}
```

In the code above, the `Clustering` method is called and then there is a loop to iterate through each cluster and call `renderPointCloud` on each cluster. The `renderPointCloud` is expecting each pcl viewer point cloud to have a unique identifier, so clusters are counted with `clusterId` and appended to the `obstCloud` string. To get different colors for each of the clusters, a list of colors is defined. Here we simply use red, blue and green.

As a bonus the number of points for each cluster is logged. This can be a helpful debugging tool later when trying to pick good min and max point values.

In this example the min points for a cluster are set to 3, and the max set to 30. The distance tolerance is also set to 1. Some time and effort will be needed to pick good hyperparameters, but many cases actually there won't be a perfect combination to always get perfect results.

![Clusters shown in different colors, red, green, and blue](https://video.udacity-data.com/topher/2019/March/5c84a20a_clusters1/clusters1.png)
>Clusters shown in different colors, red, green, and blue


### Instructions

- Define the function clusters in `pointProcessor` using the pcl document guide above for reference.
- Experiment with different hyperparameters for the clustering algorithm.
- In `environment.cpp` render the different clusters using the code sample above.


### Solution

{% include video id="4fX2d-7Ym-c" provider="youtube" %}



## 4. Implementing KD-Tree

{% include video id="OG49SnS9URs" provider="youtube" %}

A KD-Tree is a binary tree that splits points between alternating axes. By separating space by splitting regions, nearest neighbor search can be made much faster when using an algorithm like euclidean clustering. In this quiz you will be looking at a 2D example, so the the tree will be a 2D-Tree. In the first part of the quiz you will be working from `src/quiz/cluster/kdtree.h` and filling in the function `insert` which takes a 2D point represented by a vector containing two floats, and a point ID. The ID is a way to uniquely identify points and a way to tell which index the point is referenced from on the overall point cloud. To complete the `insert` function let's first talk about how a KD-Tree splits information.


### Inserting Points into the Tree

![2D points to cluster](https://video.udacity-data.com/topher/2019/March/5c84a228_2dpoints/2dpoints.png)
>2D points to cluster

The image above shows what the 2D points look like. In this simple example there are only 11 points, and there are three clusters where points are in close proximity to each other. You will be finding these clusters later.

In `src/quiz/cluster/cluster.cpp` there is a function for rendering the tree after points have been inserted into it. The image below shows line separations, with blue lines splitting x regions and red lines splitting y regions. The image shows what the tree looks like after all 11 points have been inserted, and you will be writing the code to do this over the next concepts.

![Tree separating x and y regions](https://video.udacity-data.com/topher/2019/March/5c84a240_kdtree/kdtree.png)
>Tree separating x and y regions



## 5. Inserting Points into KD-Tree

{% include video id="Va2NBXOTNlY" provider="youtube" %}

Now let’s talk about how exactly the tree is created. At the very beginning when the tree is empty, root is NULL. The point inserted becomes the root, and splits the x region. Here is what this visually looks like, after inserting the first point `(-6.2, 7)`.

![Insert first point, and split x region](https://video.udacity-data.com/topher/2019/March/5c84a25d_kdtree1/kdtree1.png)
>Insert first point, and split x region

The next point is `(-6.3, 8.4)`. Since we previously split in the x-dimension, and `-6.3` is less than `-6.2`. This Node will be created and be a part of root's left node. The point `(-6.3, 8.4)` will split the region in the y dimension.

To recap, the root was at depth 0, and split the x region. The next point became the left child of root and had a depth of 1, and split the y region.

A point at depth 2 will split the x region again, so the split dimension number can actually be calculated as `depth % 2`, where 2 is the number of dimensions we are working with. The image below shows how the tree looks after inserting the second point.

![Inserting second point, splitting y region this time](https://video.udacity-data.com/topher/2019/March/5c84a269_kdtree2/kdtree2.png)
>Inserting second point, splitting y region this time

Then here is what the tree looks like after inserting two more points `(-5.2, 7.1)` and `(-5.7, 6.3)`, and having another x split division from point `(-5.7, 6.3)`. The tree is now at depth 2.

![Inserting two more points, splitting x region again](https://video.udacity-data.com/topher/2019/March/5c84a273_kdtree4/kdtree4.png)
>Inserting two more points, splitting x region again

The image below shows so far what the tree looks like after inserting those 4 points. The labeled nodes A, B, C, D, and E are all NULL but if the next point `(7.2, 6.1)` is inserted, which of those 5 nodes will the new point node be assigned to? Remember to traverse the tree starting at the root. The depth tells you which dimension (x or y) you should use for comparison.

![Tree structure from first four points](https://video.udacity-data.com/topher/2019/March/5c84a282_kdtree5/kdtree5.png)
>Tree structure from first four points


### QUIZ QUESTION

- **Q.** Which node should the point `(7.2, 6.1)` be inserted to?

- **A.** `(7.2, 6.1)` will now be at node D.


#### Solution

{% include video id="Cah22dr0KWg" provider="youtube" %}


### Improving the Tree

{% include video id="9rL2pQLAk4o" provider="youtube" %}

Having a balanced tree that evenly splits regions improves the search time for finding points later. To improve the tree, insert points that alternate between splitting the x region and the y region evenly. To do this pick the median of sorted x and y points. For instance if you are inserting the first four points that we used above `(-6.3, 8.4)`, `(-6.2, 7)`, `(-5.2, 7.1)`, `(-5.7, 6.3)` we would first insert `(-5.2, 7.1)` since it is the median along the x axis. If there is an even number of elements the lower median is chosen. The next point to be inserted would be `(-6.2, 7)`, the median of the three points for y. This would be followed by `(-5.7, 6.3)` the lower median between the two for x, and then finally `(-6.3, 8.4)`. This ordering will allow the tree to more evenly split the region space and improve search time later.



## 6. Insert Points

### Starter Code Walkthrough

{% include video id="kYhS20w1Bnk" provider="youtube" %}

The previous concept showed how points are inserted into the tree. How about doing this in C++? Implementing a recursive helper function to insert points can be a very nice way to update Nodes. The basic idea is that the tree is traversed until the Node it arrives at is NULL, in which case a new Node is created and replaces the NULL Node. For assigning a Node, one way is to use a double pointer. You could pass in a pointer to the node, starting at root, and then when you want to replace a node you can dereference the double pointer and assign it to the newly created Node. Another way of achieving this is by using a pointer reference as well. For references, check out the code below for doing insertion in C++ but with a binary tree. The insertion for a KD-Tree will be very similar to this.


### Example of Insertion for Binary Tree

#### Double Pointer

```c++
void insert(BinaryTreeNode** node, int data) {
    if (*node == NULL)
        *node = getNewNode(data);
    else if (data < (*node)->data)
        insert(&(*node)->left, data);
    else
        insert(&(*node)->right, data);
}
```


#### Pointer Reference

```c++
void insert(BinaryTreeNode*& node, int data) {
    if (node == NULL)
        node = getNewNode(data);
    else if (data < node->data)
        insert(node->left, data);
    else
        insert(node->right, data);
}
```


### Instructions

Check out the quiz and try getting the resulting image from the previous concept, visually showing how each point is separating the x/y regions.

- In `src/quiz/cluster/kdtree.h` fill in the insert function.


### Compile/Run

- In `src/quiz/cluster`, `mkdir build`
- `cd build`
- `cmake ..`
- `make`
- `./quizCluster`


### Solution

{% include video id="CAG021S8Mco" provider="youtube" %}



## 7. Searching Points in a KD-Tree

{% include video id="4luIWnHiEJQ" provider="youtube" %}

Once points are able to be inserted into the tree, the next step is being able to search for nearby points inside the tree compared to a given target point. Points within a distance of `distanceTol` are considered to be nearby. The KD-Tree is able to split regions and allows certain regions to be completely ruled out, speeding up the process of finding nearby neighbors.

The naive approach of finding nearby neighbors is to go through every single point in the tree and compare their distances with the target, selecting point indices that fall within the distance tolerance of the target. Instead with the KD-Tree you can compare distance within a boxed square that is `2 x distanceTol` for length, centered around the target point. If the current node point is within this box then you can directly calculate the distance and see if the point id should be added to the list of nearby ids. Then you see if your box crosses over the node division region and if it does compare that next node. You do this recursively, with the advantage being that if the box region is not inside some division region you completely skip that branch.


### Instructions

- In `src/quiz/cluster/kdtree.h` fill in the `search` function.
- Verify that when the code is run, line 115 of `cluster.cpp` produces the following output:

    ```
    Test Search
    0,1,2,3,
    ```

- Experiment by using different point values in the call to `search` in line 115 of `cluster.cpp`. Use target points that are close to points in the tree. If the distance tolerance is large enough then those expected nearby point ids should be returned.


### Compile/Run

- In `src/quiz/cluster`, `mkdir build`
- `cd build`
- `cmake ..`
- `make`
- `./quizCluster`


### Solution

{% include video id="EFgF_C_tOkw" provider="youtube" %}



## 8. Euclidean Clustering

{% include video id="MnN9-_vUz2g" provider="youtube" %}

Once the KD-Tree method for searching for nearby points is implemented, its not difficult to implement a euclidean clustering method that groups individual cluster indices based on their proximity. Inside `cluster.cpp` there is a function called `euclideanCluster` which returns a vector of vector ints, this is the list of cluster indices.

To perform the clustering, iterate through each point in the cloud and keep track of which points have been processed already. For each point add it to a list of points defined as a cluster, then get a list of all the points in close proximity to that point by using the `search` function from the previous exercise. For each point in close proximity that hasn't already been processed, add it to the cluster and repeat the process of calling proximity points. Once the recursion stops for the first cluster, create a new cluster and move through the point list, repeating the above process for the new cluster. Once all the points have been processed, there will be a certain number of clusters found, return as a list of clusters.


### Pseudocode

```c++
Proximity(point,cluster):
    mark point as processed
    add point to cluster
    nearby points = tree(point)
    Iterate through each nearby point
        If point has not been processed
            Proximity(cluster)

EuclideanCluster():
    list of clusters 
    Iterate through each point
        If point has not been processed
            Create cluster
            Proximity(point, cluster)
            cluster add clusters
    return clusters
```

The EuclideanCluster is called in cluster.cpp line 123:

```c++
std::vector<std::vector<int>> clusters{ euclideanCluster(points, tree, 3.0) };
```

The image below shows the expected output results.

![Each of the three nearby clusters is colored differently, red, blue, and green.](https://video.udacity-data.com/topher/2019/March/5c84a2b1_clusterkdtree/clusterkdtree.png)
>Each of the three nearby clusters is colored differently, red, blue, and green.


### Instructions

- In `src/quiz/cluster/cluster.cpp` fill in the `euclideanCluster` function.
- Once the method is working for the 2D point example, try extending it to work with 3D point clouds by doing the same logic but including the extra third dimension.


### Compile/Run

- In `src/quiz/cluster`, `mkdir build`
- `cd build`
- `cmake ..`
- `make`
- `./quizCluster`


### Solution

{% include video id="M08ohGZFdbY" provider="youtube" %}



## 9. Bounding Boxes

Now that you know how point cloud clustering works, we will return to the main project code. In this next exercise, you will add bounding boxes to the point cloud clusters that you previously found.

![Bounding boxes around clusters](https://video.udacity-data.com/topher/2019/March/5c84a2bd_box1/box1.png)
>Bounding boxes around clusters

As a final touch, you can add bounding boxes around the clusters. The bounding box volume could also be thought of as space the car is not allowed to enter, or it would result in a collision. Your point processor already has a bounding box function all set up. This function is already predefined since it’s doing a very simple technique for generating a box. The function `BoundingBox` looks at the min and max point values of an input cloud and stores those parameters in a box struct container. To render bounding boxes around your clusters you can can add the following code inside the loop that renders clusters in `environment.cpp`.

```c++
Box box{ pointProcessor->BoundingBox(cluster) };
renderBox(viewer, box, clusterId);
```

The results of doing this are shown in the image above.


### Instructions

- In `src/environment.cpp` add the renderBox function to render boxes around clusters


### Solution

{% include video id="p-_WzLpKJj4" provider="youtube" %}



## 10. Michael on Bounding Boxes

{% include video id="kk39stQPG84" provider="youtube" %}



## 11. Extra Challenge: PCA Boxes

Some comments from the previous concept about the way bounding boxes are calculated. That method of generating bounding boxes the boxes are always oriented along the X and Y axis. This is ok if the cluster that you are looking at has its majority of points orientated along these axes , but what if the cluster was a very long rectangular object at a 45 degree angle to the X axis. The resulting bounding box would be a unnecessarily large, and would constrain your car's available space to move around. See the image below for reference.

![PCA fitted Box](https://video.udacity-data.com/topher/2019/March/5c9ab4c6_boxexample2/boxexample2.png)
>PCA fitted Box


In the above image, the bounding box on the right is more efficient, containing all the points with the minimum area required. It would be nice to take into account box rotation in the XY plane, about the Z axis. Rotation about the X or Y axes would yield weird results, since the car in the majority of situations is not concerned with the Z dimension, or has any control over Z movement.

The file containing the box struct is located in src/render/box.h and contains an additional struct called BoxQ. This struct has a quaternion member that allows rotations. Also there is an additional renderBox function in render.cpp that takes a BoxQ argument and renders the rotational box. There is a blog post about fitting the smallest possible 3D box around a 3D point cloud here. The solution in the post uses PCA, principal component analysis and includes Z axis rotations as well. A challenge problem is then to find the smallest fitting box but which is oriented flat with the XY plane.



## 12. Outro

{% include video id="sToq6R1iqtQ" provider="youtube" %}