---
title : "02.04 — Working with Real PCD"
category :
    - Sensor Fusion
tag : 
    - C++
    - https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313

toc: true
toc_sticky: true 
use_math : true
---



## 1. Load Real PCD

{% include video id="cfbvWL3q86Q" provider="youtube" %}

In the previous lessons you learned how to segment and cluster simple simulated point clouds. Now you will learn how to apply these same skills to actual point cloud data from a self-driving car. You will also learn how to do additional filtering techniques and create a pipeline to perform obstacle detection across multiple streaming pcd files. So let’s get started by loading up some actual point cloud data from a self-driving car.



## 2. Load PCD

{% include video id="Mecr0DuuwTY" provider="youtube" %}

To first load up one of the car’s recorded pcd files, you will want to create a new point processor, similar to the one we created before in the `simpleHighway` function. This time however you will be using the pcl `PointXYZI` type, the "I" stands for intensity, which will now be an additional feature for each point in the cloud. In `environment.cpp` you should create a new function called `CityBlock` that will be the same layout as the `simpleHighway` function. The arguments for `CityBlock` will be the same as `simpleHighway`, a reference to the pcl viewer. Inside the new `CityBlock` function you will create a new point processor using a `PointXYZI` template argument. You will use the point processor to load one of the car’s point clouds and then use the `renderPointCloud` function to view it. Don't forget to call `cityBlock` now instead of `simpleHighway` in the `main` function. Check out the code below for reference. All of the car’s pcd files are located in `src/sensors/data/pcd/data_1/`.

```c++
void cityBlock(pcl::visualization::PCLVisualizer::Ptr& viewer) {
    // ----------------------------------------------------
    // -----Open 3D viewer and display City Block     -----
    // ----------------------------------------------------

    ProcessPointClouds<pcl::PointXYZI>* pointProcessorI{
        new ProcessPointClouds<pcl::PointXYZI>{}
    };

    pcl::PointCloud<pcl::PointXYZI>::Ptr inputCloud{
        pointProcessorI->loadPcd("../src/sensors/data/pcd/data_1/0000000000.pcd")
    };

    renderPointCloud(viewer, inputCloud, "inputCloud");
}
```

The image below is what the results of loading and running this look like, if the color is not specified in the `renderPointCloud` function argument, it will default to using the intensity color coding. Looking around the pcd, you can see several cars parked along the sides of the road and a truck approaching to pass the ego car on the left side. Your goal will be to fit bounding boxes around these cars and the passing truck, so then your system could later use that information in its path planner, trying to avoid any collisions with those obstacles.

![Loading pcd from from a self-driving car](https://video.udacity-data.com/topher/2019/March/5c8598b6_pcd2/pcd2.png)
>Loading pcd from from a self-driving car.


### Instructions

- Create `cityBlock` function as shown above
- Replace the `simpleHighwaycall` in `main` with `cityBlock`
- Compile/Run, The output will look like the image above
- Orbit and Zoom around the point cloud

{% include video id="Y7kFT1dW6iQ" provider="youtube" %}



## 3. Challenges with Real World Lidar

{% include video id="3WXluu7PyTA" provider="youtube" %}



## 4. Downsampling

{% include video id="ry17-7srqlE" provider="youtube" %}



## 5. Filtering with PCL

### Filtering with PCL

{% include video id="yLG_OKDW9Fs" provider="youtube" %}

The first thing you might notice when looking at the previous loaded point cloud is it’s quite high resolution and spans a pretty far distance. You want your processor pipeline to be able to digest point cloud as quickly as possible, so you will want to filter the cloud down. Here are the two methods that will be used to do this.


### Voxel Grid

Voxel grid filtering will create a cubic grid and will filter the cloud by only leaving a single point per voxel cube, so the larger the cube length the lower the resolution of the point cloud.

{% include video id="NxdZAcDhfVQ" provider="youtube" %}


### Region of Interest

A boxed region is defined and any points outside that box are removed.

To apply these methods you will fill in the point process function `FilterCloud`. The arguments to this function will be your input cloud, voxel grid size, and min/max points representing your region of interest. The function will return the downsampled cloud with only points that were inside the region specified. To get started check out the documentation from PCL for [voxel grid filtering](https://pointclouds.org/documentation/tutorials/voxel_grid.html) and [region of interest](https://pointclouds.org/documentation/classpcl_1_1_statistical_multiscale_interest_region_extraction.html).

{% include video id="FRIm7Ya8Xy8" provider="youtube" %}


### Results

To apply the filtering function, back in `cityBlock` call the `pointProcessor FilterCloud` function with the loaded pcd.

```c++
// Experiment with the ? values and find what works best
filterCloud = pointProcessorI->FilterCloud(inputCloud, ? , Eigen::Vector4f(? , ? , ? , 1), Eigen::Vector4f(? , ? , ? , 1));
renderPointCloud(viewer, filterCloud, "filterCloud");
```

In the filter cloud image below you can now see that the point resolution is much lower than the original, and it cropped everything inside the box points that were specified. It’s important to experiment and play around with the filter input hyper parameters. voxel size should be large enough to help speed up the processing but not so large that object definition is completely lost. For picking a good region, try having a good amount of space in front of the car so it could react quickly in time to any obstacles moving towards it. Also for the sides try to cover at least the width of the road. What’s most important is obstacles that we want to detect are inside the region. Also setting camera angles in `environment.cpp` can help pick a good region of interests. This way you can easily set the camera to have a top down overview or a side overview. One last thing is it would be beneficial to remove points that are hitting the roof of the ego car. You can use a pcl `CropBox` to find the roof point indices and then feed those indices to a pcl `ExtractIndices` object to remove them (similar to what your segmentation algorithm used to extract points). The `renderBox` function can be really helpful as well for figuring out how big boxes will look in the scene.

![Region and voxel grid filtering](https://video.udacity-data.com/topher/2019/March/5c8598cc_filtered/filtered.png)
>Region and voxel grid filtering


### Instructions

- Fill in the `FilterCloud` function in `pointProcessor`
- Call `FilterCloud` from `cityBlock` and experiment with hyperparameters
- (Optional) Remove ego car roof points
- Observe the results

{% include video id="5aMtbD3criU" provider="youtube" %}


### Solution

{% include video id="62Bk64oquzY" provider="youtube" %}



## 6. Steps For Obstacle Detection

Now that the pcd is filtered you are ready to deploy the same segmentation and clustering techniques that we applied before in the previous lessons, now using the new intensity point processor inside `cityBlock`.

### Step 1. Segment the filtered cloud into two parts, road and obstacles.

{% include video id="CRvOO45z-1I" provider="youtube" %}

After you filter the point cloud the next step is to segment it. The image below shows the filtered point cloud segmented (road in green), (obstacles in red), with points only in the filtered region of interest. The image also displays a purple box showing the space where the car's roof points were contained, and removed.

![Segmented point clouds](https://video.udacity-data.com/topher/2019/March/5c8598e5_seg2/seg2.png)
>Segmented point clouds. The purple box shows where ego car roof point were removed.


### Step 2. Cluster the obstacle cloud

{% include video id="Ge0262WG8Kg" provider="youtube" %}

Next you cluster the obstacle cloud based on the proximity of neighboring points. The image below shows the clusters in cycled colors of red, yellow, and blue. In that image we see that the oncoming truck is actually broken up into two colors, front and back. This illustrates the challenges with clustering based on proximity, the gap between the front of the truck and the back of the truck is large enough so that they look separate. You might think to fix this by increasing the distance tolerance, but you can also see that the truck is getting really close to one of the side parked cars. Increasing the distance tolerance would run the risk of the truck and parked car being grouped together.

![Clustering the point cloud](https://video.udacity-data.com/topher/2019/March/5c8598f5_cluster2/cluster2.png)
>Clustering the point cloud. Different cluster shown in cycled colors, red, yellow, and blue.


### Step 3. Find bounding boxes for the clusters

Finally you place bounding boxes around the individual clusters. Since all the detectable vehicles in this scene are along the same axis as our car, the simple already set up bounding box function in point processor should yield good results.

![Bounding boxes around cluster obstacles](https://video.udacity-data.com/topher/2019/March/5c859900_box2/box2.png)
>Bounding boxes around cluster obstacles.

Awesome! Congratulations for making it through the point process pipeline on a real pcd file. Once you are happy with your results from a single frame, let's look at processing a stream of frames.



## 7. Stream PCD

![Playing back the pcd files](https://video.udacity-data.com/topher/2019/March/5c85991d_pcdstream/pcdstream.gif)
>Playing back the pcd files.

In the previous concept you were able to process obstacle detections on a single pcd file, now you are going to be using that same processing pipeline on multiple pcd files. To do this you can slightly modify the previous used `cityBlock` function from `environment.cpp` to support some additional arguments. Now, you will be passing in the point processor to the `cityBlock` function, this is because you don't want to have to recreate this object at every frame. Also the point cloud input will vary from frame to frame, so input point cloud will now become an input argument for `cityBlock`. The `cityBlock` function header should now look like this, and you no longer create the point processor or load a point cloud from inside the function.


### `cityBlock` new Function Signature

```c++
void cityBlock(
	pcl::visualization::PCLVisualizer::Ptr& viewer,
	ProcessPointClouds<pcl::PointXYZI>* pointProcessorI,
	const pcl::PointCloud<pcl::PointXYZI>::Ptr& inputCloud) {}
```

Notice that in the function header you can optionally make `inputCloud` a constant reference by doing `const` and `&` at the end of the variable definition. You don't have to do this but you are not actually changing the `inputCloud` at all, just using it as an input for your point processor function. The benefit of using a constant reference is better memory efficiency, since you don't have to write to that variable's memory, just read from it, so it's a slight performance increase. If you do make this a const reference though, make sure not to modify it, or else you will get a compile error.


### Code inside `main`

So now instead of creating your point processor, and loading pcl files from inside `cityBlock` you will do this inside the `main` function in `environment.cpp` right after where the pcl viewer camera position is set up.

```c++
ProcessPointClouds<pcl::PointXYZI>* pointProcessorI{
	new ProcessPointClouds<pcl::PointXYZI>{}
};

std::vector<boost::filesystem::path> stream{
	pointProcessorI->streamPcd("../src/sensors/data/pcd/data_1")
};

auto streamIterator{ stream.begin() };

pcl::PointCloud<pcl::PointXYZI>::Ptr inputCloudI;
```

In the code above, you are making use of a new method from point processor called, `streamPcd`. You tell `streamPcd` a folder directory that contains all the sequentially ordered pcd files you want to process, and it returns a chronologically ordered vector of all those file names, called `stream`. You can then go through the `stream` vector in a couple of ways, one option is to use an iterator. At the end of the above code block, a variable for the input point cloud is also set up.


### PCL Viewer Update Loop

The final thing to look at is the pcl viewer run cycle which is down at the bottom of `envrionment.cpp`. While the pcl viewer hasn't stopped, you want to process a new frame, do obstacle detection on it, and then view the results. Let's see how to set up this pcl viewer run cycle method below.

```c++
while (!viewer->wasStopped()) {

    // Clear viewer.
    viewer->removeAllPointClouds();
    viewer->removeAllShapes();

    // Load pcd and run obstacle detection process.
    inputCloudI = pointProcessorI->loadPcd((*streamIterator).string());
    cityBlock(viewer, pointProcessorI, inputCloudI);

    ++streamIterator;
    if (streamIterator == stream.end())
        streamIterator = stream.begin();

    viewer->spinOnce();
}
```

The first thing the above method does is clear any previous rendered point clouds or shapes. Next it loads up your point cloud using your point processor and stream iterator. Then it calls your `cityBlock` function, and updates the iterator. If the iterator hits the end of the vector it simply sets it back to the beginning and that's it. The `viewer->spinOnce()` call controls the frame rate, by default it waits 1 time step, which would make it run as fast as possible. Depending on how timing efficient your obstacle detection functions were set up the faster the viewer's frame rate will be. If you want to check out the input pcd data at the fastest rate then run the code above and only run a single `renderPointCloud` on the input cloud inside `cityBlock`. Let's check out the results of the streaming pcd viewer below.

![](https://video.udacity-data.com/topher/2019/March/5c85992d_pcdstreamdetection/pcdstreamdetection.gif)


### Instructions

- Modify `environment.cpp` with the above changes
- Call `cityBlock` to perform obstacle detection on each frame


### Solution

{% include video id="gycfszhCttY" provider="youtube" %}



## 8. Lidar Obstacle Detection Project

{% include video id="lGbHW8SMu24" provider="youtube" %}

In this project you will take everything that you have learned for processing point clouds, and use it to detect car and trucks on a narrow street using lidar. The detection pipeline should follow the covered methods, filtering, segmentation, clustering, and bounding boxes. Also the segmentation, and clustering methods should be created from scratch using the previous lesson’s guidelines for reference. The finished result will look like the image below, placing bounding boxes around all obstacles on the road.

![Lidar Obstacle Detection](https://video.udacity-data.com/topher/2019/March/5c859943_obstacledetectionfps/obstacledetectionfps.gif)
>Lidar Obstacle Detection.



## 9. Tracking and Challenge Problem

### Tracking Discussion

You have made it through the entire lidar obstacle detection process. You are able to stream back multiple pcd files and perform filtering, segmentation, clustering, and bounding box detection. Now that you are able to detect obstacles in single frames, you can make your pipeline even more robust by tracking detections over the history of frames. You can create associations between detections in frames and use that to track objects.

One way to create associations between two different frames is by how close in proximity two detections are to each other and how similar they look. There are also a lot more filtering procedures that you can explore, such as looking at detection that are seen in consecutive frames before they are considered. You could also filter based on bounding boxes, their volume and shapes. By deploying tracking methods and associations you could try to dynamically build the shapes of obstacles. Examples of this might be, maybe you see the backside of a long truck, the lidar only first sees the back of the truck. Then later you drive past the truck. letting the lidar see the trucks side. There are many ways to keep exploring and making the detection process more robust.

![Challenge problem](https://video.udacity-data.com/topher/2019/March/5c859977_challengeset/challengeset.gif)
>Challenge problem: Track a bicyclist riding in front of the car.


### Challenge Problem

If you are up for an additional challenge check out `src/sensors/data/pcd/data_2` to see how well you can detect/track a bicyclist riding in front of the car, along with detecting/tracking the other surrounding obstacles in the scene.



## 10. Outro

{% include video id="1_kZ0lgWy5U" provider="youtube" %}