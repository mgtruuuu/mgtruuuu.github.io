---
title : "01.01 — Introduction to Lidar and Point Clouds - 1"
category :
    - Sensor Fusion
tag : 
    - C++
    - https://www.udacity.com/

toc: true  
toc_sticky: true 
use_math : true
---



## 1. Welcome!

{% include video id="f4bx0tzpBBU" provider="youtube" %}


In this course we will be talking about sensor fusion, which is the process of taking data from multiple sensors and combining it to give us a better understanding of the world around us. We will mostly be focusing on two sensors, lidar, and radar. By the end you will be fusing the data from these two sensors to track multiple cars on the road, estimating their positions and speed.

In Sensor Fusion, by combining lidar’s high resolution imaging with radar's ability to measure velocity of objects we can get a better understanding of the surrounding environment than we could using one of the sensors alone. Before starting to fuse multiple sensor information together though, you will first go through the process of getting obstacle positions from raw lidar data. So to get started let’s check out lidar sensors and the high resolution point clouds they generate.



## 2. MBRDNA Introduction

Throughout the Lidar course, you will have perspectives about Lidar from Michael Maile. Michael manages the sensor fusion team at [MBRDNA](https://mbrdna.com/). In the next video, Michael will tell you a little bit about himself and his role at MBRDNA.

{% include video id="UkDzLSIVQ0U" provider="youtube" %}



## 3. What is Lidar?

{% include video id="IGODQteik6M" provider="youtube" %}



## 4. Lidar Sensors


{% include video id="JoD31NDeDYA" provider="youtube" %}

Lidar sensing gives us high resolution data by sending out thousands of laser signals. These lasers bounce off objects, returning to the sensor where we can then determine how far away objects are by timing how long it takes for the signal to return. Also we can tell a little bit about the object that was hit by measuring the intensity of the returned signal. Each laser ray is in the infrared spectrum, and is sent out at many different angles, usually in a 360 degree range. While lidar sensors gives us very high accurate models for the world around us in 3D, they are currently very expensive, upwards of $60,000 for a standard unit.

- The Lidar sends thousands of laser rays at different angles.
- Laser gets emitted, reflected off of obstacles, and then detected using a receiver.
- Based on the time difference between the laser being emitted and received, distance is calculated.
- Laser intensity value is also received and can be used to evaluate material properties of the object the laser reflects off of.

![Velodyne lidar sensors, with HDL 64, HDL 32, VLP 16 from left to right. The larger the sensor, the higher the resolution.](https://video.udacity-data.com/topher/2019/March/5c81e202_lidar-velodyne/lidar-velodyne.png)

Here are the specs for a HDL 64 lidar. The lidar has 64 layers, where each layer is sent out at a different angle from the z axis, so different inclines. Each layer covers a 360 degree view and has an angular resolution of 0.08 degrees. On average the lidar scans ten times a second. The lidar can pick out objects up to 120M for cars and foliage, and can sense pavement up to 50M.

![VLP 64 schematic, showing lidar emitters, receivers, and housing.](https://video.udacity-data.com/topher/2019/March/5c82b49a_hdl-64e/hdl-64e.png)

![VLP Sensor Specifications](https://video.udacity-data.com/topher/2019/March/5c82b609_vlp-sensor-specs/vlp-sensor-specs.png)



## 5. What is a Point Cloud?

{% include video id="OD45m2YFsU0" provider="youtube" %}



## 6. Point Clouds

### PCD Files

{% include video id="ptExV1DFIbo" provider="youtube" %}

Let’s dive into how lidar data is stored. Lidar data is stored in a format called Point Cloud Data (PCD for short). A .pcd file is a list of (x,y,z) cartesian coordinates along with intensity values, it’s a single snapshot of the environment, so after a single scan. That means with a VLP 64 lidar, a pcd file would have around 256,000 (x,y,z,i) values.

![PCD of a city block with parked cars, and a passing van. Intensity values are being shown as different colors. The big black spot is where the car with the lidar sensor is located.](https://video.udacity-data.com/topher/2019/March/5c82b6ce_pcd2/pcd2.png)
>PCD of a city block with parked cars, and a passing van. Intensity values are being shown as different colors. The big black spot is where the car with the lidar sensor is located.


### PCD Coordinates

{% include video id="nRjGYqhl9JU" provider="youtube" %}

The coordinate system for point cloud data is the same as the car’s local coordinate system. In this coordinate system the x axis is pointing towards the front of the car, and the y axis is pointing to the left of the car. Also since this coordinate system is right-handed the z axis points up above the car.

![PCD Coordinate System](https://video.udacity-data.com/topher/2019/March/5c8fd89d_pcd-coordinates/pcd-coordinates.png)
>PCD Coordinate System


## 7. Point Cloud Tools

{% include video id="JrqJ2ZO3agY" provider="youtube" %}



## 8. The Point Cloud Library (PCL)

### The PCL Library

{% include video id="Krl-LJi3Sis" provider="youtube" %}

In this module you will be working on processing point cloud data to find obstacles. All the code will be done in a C++ environment, so some familiarity with C++ will definitely be helpful. PCL is an open source C++ library for working with point clouds. You will be using it to visualize data, render shapes, and become familiar with some of its built in processing functions. Some documentation for PCL can be found [here](http://pointclouds.org/).

PCL is widely used in the robotics community for working with point cloud data, and there are many tutorials available online for using it. There are a lot of built in functions in PCL that can help to detect obstacles. Built in PCL functions that will be used later in this module are Segmentation, Extraction, and Clustering.


## 9. Using Lidar on an Autonomous Vehicle

{% include video id="i1C58WbWufY" provider="youtube" %}



## 10. The Course Starter Code

### Starter Repo Structure

{% include video id="yVdyzmStvFI" provider="youtube" %}

All the code for doing lidar obstacle detection is contained in a GitHub repository. The classroom has workspace environments that already include all the dependencies for getting started right way. You can also clone the repo, and use the README to get started on your own machine as well. Here is the [link](https://github.com/udacity/SFND_Lidar_Obstacle_Detection).

You will mostly be working out of two main files, which are `environment.cpp` and `processPointClouds.cpp`. The `environment.cpp` file contains the `main` function and will generate the runnable executable. The `processPointClouds.cpp` file will contain all your function placeholders to process the pcd.

There are some other files worth mentioning, like `sensors/lidar.h`, which simulates lidar sensing and creates point cloud data. Also `render.cpp` and `render.h` which have functions for rendering objects onto the screen.


### Code Structure

- Top-level CMakeLists.txt
- Readme
- src
    - render
        - box.h - this file has the struct definitions for box objects
        - render.h
        - render.cpp - this file, along with the header, define the classes and methods for rendering objects.
    - sensors
        - data - this directory contains pcd data used in the course.
        - lidar.h - has functions using ray casting for creating pcd.
    - environment.cpp - the main file for using pcl viewer and processing and visualizing pcd.
    - processPointClouds.h
    - processPointClouds.cpp - Functions for filtering, segmenting, clustering, boxing, loading, and saving pcd.


### Starter Repo Walkthrough

{% include video id="LgoJWYESisI" provider="youtube" %}



## 11. Compiling the Lidar Simulator

### Compilation Instructions


### Compilation Walkthrough

{% include video id="kj3PLhCVuPg" provider="youtube" %}

- In the terminal workspace below, click on Desktop button on lower right. A new virtual Desktop will open up in a separate browser tab.
- Click on Terminator to load up work space desktop terminal.
- From the terminal, go to the project root directory, cd /home/workspace/SFND-Lidar-Obstacle-Detection.
- Create a new directory from the project root named build with the following command: mkdir build.
- Then go into the build directory: cd build.
- Run cmake pointing to the CMakeLists.txt in the root: cmake ... If everything went well you should see something like

    <pre><code class="lang-bash">-- Configuring <span class="hljs-keyword">done</span>
-- Generating <span class="hljs-keyword">done</span>
-- Build files have been written to: /your directory/simple_highway/build
</code></pre>

- If cmake was successful, then from inside build run make: make
- If make built target environment 100%, it will have generated an executable called environment. This build process is defined from the CMakeLists.txt file.



## 12. Running the Simulator


{% include video id="PBNYLldDV3w" provider="youtube" %}


### Instructions

Once you have built an executable file, you can launch it by doing ./environment. Now you should see a window popping up that looks like the image above.

Here you have a simple highway simulator environment with the ego car in green in the center lane (thats your car), and the other traffic cars in blue. Everything is rendered using PCL with simple boxes, lines, and colors. You can move around your environment with the mouse. Try holding the left mouse button to orbit around the scene. You can also pan around the scene by holding the middle mouse button and moving. To zoom, use the middle scroll mouse button or the right mouse button while moving.


### Recap

- Using terminator in the virtual desktop, run the executable from the build directory using ./environment.
- You should see a 3D popup window with the road and cars.
- You can move around the environment.
- Zoom: hold the right mouse key and move the mouse forward/backwards, or use your mouse scroller.
- Pan: Hold down the middle mouse button (the scroller) and move the mouse.
- Rotate: Hold the left mouse button and move the mouse.

![](https://video.udacity-data.com/topher/2019/March/5c82b74b_environment/environment.png)


### Try Running the Simulator Yourself