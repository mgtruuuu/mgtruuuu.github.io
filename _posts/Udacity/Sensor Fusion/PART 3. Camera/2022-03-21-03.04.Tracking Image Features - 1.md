---
title : "03.04 — Tracking Image Features - 1"
category :
    - Sensor Fusion
tag : 
    - C++
    - https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313

toc: true  
toc_sticky: true 
use_math : true
---



## 1. Overview

### Overview

In this lesson, you will learn how to identify and track reliable and stable features through a sequence of images. As you will soon see, this lesson is one of the most challenging in the entire course:

- First, you need to learn about intensity gradients and image filtering basics and the properties required to describe suitable keypoints (also called as the **point of interest**).

- Second, once you are familiar with how intensity can be used to describe noteworthy points mathematically, we will take a look at one of the most well-known keypoint detection algorithms, the Harris detector. Even though it is several decades old, the Harris detector is still used in many applications today, and it can be counted among the most famous algorithms in computer vision.

- Third, I will give you an overview of contemporary keypoint detectors and their properties, such as robustness under rotation of the image content and other transformations that occur in practice.

- Once you are familiar with locating keypoints in an image, the next step will be to track these from one image to the next so that we can use them to reconstruct vehicle motion. To do so, we need to employ a concept called “**descriptors**”, which describe (hence the name) the surrounding area around a keypoint in a unique manner. It can be located again in the next image provided by the camera. There are several types of descriptors available, and you will get a thorough overview of these in the fourth part of this lesson.

- Lastly, once you know about detectors and descriptors, you will need suitable measures that you can use to describe the similarity between keypoints. Therefore, I will give you an overview of the essential methods to do this in the fifth section.

As I wrote at the beginning of this intro: Computer vision is a complex topic and the task of tracking image features is always a challenge. But understanding the basics and the math behind it gives you a clear edge towards others, which choose to use a set of algorithms from a computer vision library without understanding what’s going on behind the scenes. Please invest the effort to work through this lesson properly - and the chances are that it will be fun when you start seeing your first results in the code examples.

{% include video id="2E9TNZlCGX8" provider="youtube" %}



## 2. Intensity Gradient and Filtering

### Why is this section important for you?

Locating keypoints (or points of interest) in an image is the first step in the tracking process. It is the first step in the tracking pipeline you have already read about in the introduction to this lesson. The basis for the majority of keypoint types is the distribution of brightness information over the image. Places where it changes rapidly are said to have a high intensity gradient and might therefore be suitable keypoints. This section will introduce you to the concept and help you build the basis for a proper understanding of the next sections in this lesson.

As discussed in the previous lesson, a camera is not able to measure distance to an object directly. However, for our collision avoidance system, we can compute time-to-collision based on relative distance ratios on the image sensor instead. To do so, we need a set of locations on the image plane which can serve as stable anchors to compute relative distances between them. This section discussed how to locate such anchor locations - or **keypoints** in an image.

Take a look at the three patches in the following figure which have been extracted from an image of a highway driving scene. The grid shows the borders of individual pixels. How would you describe meaningful locations within those patches that could be used as keypoints?

![Three image patches extracted from a highway driving scene](https://video.udacity-data.com/topher/2019/April/5cbf9117_new-group/new-group.jpg)
>Three image patches extracted from a highway driving scene

In the leftmost patch, there is a distinctive contrast between bright and dark pixels which resembles a line from the bottom-left to the upper-right. The patch in the middle resembles a corner formed by a group of very dark pixels in the upper-left. The rightmost patch looks like a bright blob that might be approximated by an ellipse.

In order to precisely locate a keypoint in an image, we need a way to assign them a unique coordinate in both `x` an `y`. Not all of the above patches lend themselves to this goal. Both the corner as well as the ellipse can be positioned accurately in `x` and `y`, the line in the leftmost image can not.

![Three images patches with lines drawn](https://video.udacity-data.com/topher/2019/April/5cbf9168_new-group-kopieren/new-group-kopieren.jpg)
>Three images patches with lines drawn

In the following, we will thus concentrate on detecting corners in an image. In a later section, we will also look at detectors who are optimized for blob-like structures, such as the SIFT detector.


### The Intensity Gradient

In the examples above, the contrast between neighboring pixels contains the information we need in order to precisely locate e.g. the corner in the middle patch, we do not need to know its color but instead we require the color difference between the pixels that form the corner to be as high as possible. An ideal corner would consist of only black and white pixels.

The figure below shows the **intensity profile** of all pixels along the red line in the image as well as the **intensity gradient**, which is the derivative of image intensity.

![intensity profile of image pixels along the red line](https://video.udacity-data.com/topher/2019/April/5cbf91b8_intensity-and-derivative/intensity-and-derivative.jpg)
>Intensity profile of image pixels along the red line  
Image adapted from the presentation titled "Finding Edges and Straight Lines" by Derek Hoiem, University of Illinois, 2010

It can be seen that the intensity profile increases rapidly at positions where the contrast between neighboring pixels changes significantly. The lower part of the street lamp on the left side and the dark door show a distinct intensity difference to the light wall. If we wanted to assign unique coordinates to the pixels where the change occurs, we could do so by looking at the derivative of the intensity, which is the blue gradient profile you can see below the red line. Sudden changes in image intensity are clearly visible as distinct peaks and valleys in the gradient profile. **If we were to look for such peaks not only from left to right but also from top to bottom, we could look for points which show a gradient peak both in horizontal and in vertical direction and choose them as keypoints with both `x` and `y` coordinates.** In the example patches above, this would work best for the corner, whereas an edge-like structure would have more or less identical gradients at all positions with no clear peak in `x` and `y`.

Based on the above observations, the first step into keypoint detection is thus the computation of a gradient image. Mathematically, the gradient is the partial derivative of the image intensity into both `x` and `y` direction. The figure below shows the intensity gradient for three example patches. The gradient direction is represented by the arrow.

![Intensity gradient for three patches and partial derivatives](https://video.udacity-data.com/topher/2021/March/605b3a75_sf-imageupdates-aug2020-lesson-4.001/sf-imageupdates-aug2020-lesson-4.001.png)
>Intensity gradient for three patches and partial derivatives

In equations (1) and (2), the intensity gradient is approximated by the intensity differences between neighboring pixels, divided by the distance between those pixels in x- and y-direction. Next, based on the intensity gradient vector, we can compute both the direction as well as the magnitude as given by the following equations:

![Equations for gradient direction and magnitude](https://video.udacity-data.com/topher/2020/September/5f5b78a1_sf-imageupdates-aug2020-lesson-4.002/sf-imageupdates-aug2020-lesson-4.002.png)
>Equations for gradient direction and magnitude

There are numerous ways of computing the intensity gradient. The most straightforward approach would be to simply compute the intensity difference between neighboring pixels. This approach however is extremely sensitive to noise and should be avoided in practice. Further down in this section, we will look at a well-proven standard approach, the **Sobel operator**.


## 3. Exercise - Image Filters and Gaussian Smoothing

### Image Filters and Gaussian Smoothing

Before we further discuss gradient computation, we need to think about noise, which is present in all images (except artificial ones) and which decreases with increasing light intensity. To counteract noise, especially under low-light conditions, a smoothing operator has to be applied to the image before gradient computation. Usually, a Gaussian filter is used for this purpose which is shifted over the image and combined with the intensity values beneath it. In order to parameterize the filter properly, two parameters have to be adjusted:

>>> parameterize? kernel size?? 

- The **standard deviation**, which controls the spatial extension of the filter in the image plane. *The larger the standard deviation, the wider the area which is covered by the filter.*

- The **kernel size**, which defines how many pixels around the center location will contribute to the smoothing operation.

The following figure shows three Gaussian filter kernels with varying standard deviations.

![](https://video.udacity-data.com/topher/2019/April/5cbf938c_jcuj2/jcuj2.png)

Gaussian smoothing works by assigning each pixel a weighted sum of the surrounding pixels based on the height of the Gaussian curve at each point. The largest contribution will come from the center pixel itself, whereas the contribution from the pixels surroundings will decrease depending on the height of the Gaussian curve and thus its standard deviation. It can easily be seen that the contribution of the surrounding pixels around the center location increases when the standard deviation is large (left image).

Applying the Gaussian filter (or any other filter) works in four successive steps which are illustrated by the figure below:

1. Create a filter kernel with the desired properties (e.g. Gaussian smoothing or edge detection)
2. Define the anchor point within the kernel (usually the center position) and place it on top of the first pixel of the image.
3. Compute the sum of the products of kernel coefficients with the corresponding image pixel values beneath.
4. Place the result to the location of the kernel anchor in the input image.
5. Repeat the process for all pixels over the entire image.

The following figure illustrates the process of shifting the (yellow) filter kernel over the image row by row and assigning the result of the two-dimensional sum `H(x,y)` to every pixel location. Note that `x` and `y` express a position in the coordinate system of the image while `i` and `j` are coordinates within the filter kernel. Also, `a_i` and `a_j` are the coordinates of the anchor point $a$ of the filter kernel. If you are unsure why both values need to be subtracted from `x` and `y`, try to insert the values for a simple 3x3 filter kernel by hand and see what the subtraction does.

![](https://video.udacity-data.com/topher/2020/September/5f5b79fa_3/3.png)

A filter kernel for Gaussian smoothing is shown in the next figure. The figure shows a *discrete* filter kernel with a central anchor point (at `[4, 4]`) corresponding to the maximum of the Gaussian curve and with decreasing values towards the edges in a (approximately) circular shape.

![A sample Gaussian matrix](https://video.udacity-data.com/topher/2020/September/5f620626_screenshot-2020-09-16-at-6.02.42-pm/screenshot-2020-09-16-at-6.02.42-pm.png)
>A sample Gaussian matrix. Source - [Wikipedia](https://en.wikipedia.org/wiki/Gaussian_blur#Sample_Gaussian_matrix).


### Exercise

{% include video id="g_sYIKjjdv4" provider="youtube" %}

The following code uses the function `cv::filter2D` to apply the filter above to an image. Run the code and figure out why the output image does not look as it is supposed to look after applying the smoothing filter. Once you know why, make the necessary changes and run the code again until you see a slightly blurred image. You can compile using `cmake` and `make` as before, and run the code using the generated `gaussian_smoothing` executable.

**Note:** To build and run the code below, use the following steps:

- Go to the virtual Desktop by clicking the Desktop button. You can use Terminator or a VSCode terminal to run the following commands from the /home/workspace/gradient_filtering directory:

    ```
    mkdir build && cd build
    cmake ..
    make
    ```

- Run the executable from the `build` directory with the command: `./gaussian_smoothing`

- Other files, `home/workspace/gradient_filtering/src/magnitude_sobel.cpp` and `home/workspace/gradient_filtering/src/gradient_sobel.cpp` will be used in the next exercise.

>**Hint** - See this discussion on [how to implement Gaussian blur](https://computergraphics.stackexchange.com/questions/39/how-is-gaussian-blur-implemented)?

The code above is meant to illustrate the principle of filters and of Gaussian blurring. In your projects however, you can (and should) use the function `cv::GaussianBlur`, which lets you change the standard deviation easily without having to adjust the filter kernel.



## 4. Exercise - Computing the Intensity Gradient

### Computing the Intensity Gradient

After smoothing the image slightly to reduce the influence of noise, we can now compute the intensity gradient of the image in both `x` and `y` direction. In the literature, there are several approaches to gradient computation to be found. Among the most famous is the **Sobel operator** (proposed in 1968), but there are several others, such as the **Scharr operator**, which is optimized for rotational symmetry.

The Sobel operator is based on applying small integer-valued filters both in horizontal and vertical direction. The operators are 3x3 kernels, one for the gradient in `x` and one for the gradient in `y`. Both kernels are shown below.

![](https://video.udacity-data.com/topher/2020/September/5f5b7a82_4/4.png)

In the following code, one kernel of the Sobel operator is applied to an image. Note that it has been converted to grayscale to avoid computing the operator on each color channel. This code can be found in the `gradient_sobel.cpp` file in the desktop workspace above. You can run the code by using the `gradient_sobel` executable.

```c++
// Load image from file.
cv::Mat img{ cv::imread("./img1.png") };

// Convert image to grayscale.
cv::Mat imgGray;
cv::cvtColor(img, imgGray, cv::COLOR_BGR2GRAY);

// Create filter kernel.
float sobel_x[9]{
    -1, 0, +1, 
    -2, 0, +2, 
    -1, 0, +1
};
cv::Mat kernel_x{ 3, 3, CV_32F, sobel_x };

// Apply filter.
cv::Mat result_x;
cv::filter2D(imgGray, result_x, -1, kernel_x, cv::Point{ -1, -1 }, 0.0, cv::BORDER_DEFAULT);

// Show result.
string windowName{ "Sobel operator (x-direction)" };
cv::namedWindow(windowName, 1); // Create window.
cv::imshow(windowName, result_x);
cv::waitKey(0);                 // Wait for keyboard input before continuing.
```

The resulting gradient image is shown below. It can be seen that areas of strong local contrast such as the cast shadow of the preceding vehicle leads to high values in the filtered image.

![Gradient image with S_x filter](https://video.udacity-data.com/topher/2019/April/5cbf9604_draggedimage-6/draggedimage-6.png)
>Gradient image with S_x filter

Note that in the above code, only the `S_x` filter kernel has been applied for now, which is why the cast shadow only shows in x direction. Applying `S_y` to the image yields the following result:

![Gradient image with S_x and S_y filter](https://video.udacity-data.com/topher/2019/April/5cbf96e0_draggedimage-7/draggedimage-7.png)
>Gradient image with `S_x` and `S_y` filter


### Exercise

{% include video id="kQNTa0x8CDI" provider="youtube" %}

Based on the image gradients in both `x` and `y`, compute an image that contains the gradient magnitude according to the equation at the beginning of this lesson **(refer to the equations for gradient direction and magnitude)** for every pixel position. Also, apply different levels of Gaussian blurring before applying the Sobel operator and compare the results.

You can use the `magnitude_sobel.cpp` file in the desktop workspace below for your solution, and after `make`, you can run the code using the `magnitude_sobel` executable.

The result should look something like this, with the noise in the road surface being almost gone due to smoothing:

![](https://video.udacity-data.com/topher/2019/April/5cbf97b8_draggedimage-9/draggedimage-9.png)



## 5. Harris Corner Detection

### Why is this section important for you?

The Harris corner detector is based on detecting locations in an image that show strong intensity gradients, which you have learned about in the last section. Understanding the Harris detector will not only familiarize yourself with one of the most well-known algorithms in computer vision but also help you sharpen your eye to gauge whether a particular image sequence will be suitable for keypoint tracking.

{% include video id="WL7Ybk5rm30" provider="youtube" %}


### Local Measures of Uniqueness

The idea of keypoint detection is to detect a unique structure in an image that can be precisely located in both coordinate directions. As discussed in the previous section, corners are ideally suited for this purpose. To illustrate this, the following figure shows an image patch which consists of line structures on a homogeneously colored background. A red arrow indicates that no unique position can be found in this direction. The green arrow expresses the opposite. As can be seen, the corner is the only local structure that can be assigned a unique coordinate in `x` and `y`.

![](https://video.udacity-data.com/topher/2020/September/5f5b82ff_5/5.png)

In order to locate a corner, we consider how the content of the window would change when shifting it by a small amount. For case (a) in the figure above, there is no measurable change in any coordinate direction at the current location of the red window W whereas for (b), there will be significant change into the direction orthogonal to the edge and no change when moving into the direction of the edge. In case of (c), the window content will change in any coordinate direction.

The idea of locating corners by means of an algorithm is to find a way to detect areas with a significant change in the image structure based on a displacement of a local window W. Generally, a suitable measure for describing change mathematically is the **sum of squared differences** (**SSD**), which looks at the deviations of all pixels in a local neighborhood before and after performing a coordinate shift. The equation below illustrates the concept.

![](https://video.udacity-data.com/topher/2020/September/5f5b8328_6/6.png)

**After shifting the Window `W` by an amount `u` in x-direction and `v` in y-direction the equation sums up the squared differences of all pixels within `W` at the old and at the new window position.** In the following we will use some mathematical transformations to derive a measure for the change of the local environment around a pixel from the general definition of the SSD.

In the first step, based on the definition of `E(u,v)` above, we will at first make a Taylor series expansion of `I( x+u, y+v )`. For small values of u and v, the first-order approximation is sufficient, which leads to the following expression.

![](https://video.udacity-data.com/topher/2020/September/5f5b8368_7/7.png)

The derivation of the image intensity `I` both in x- and y-direction is something you have learned in the previous section already - this is simply the intensity gradient . From this point on, we will use the shorthand notation shown above to express the gradient.

In the second step, we will now insert the approximated expression of `I(x+u, y+v)` into the SSD equation above, which simplifies to the following form:

![](https://video.udacity-data.com/topher/2020/September/5f5b8381_8/8.png)

The result of our mathematical transformations is a matrix $H$, which can now be conveniently analyzed to locate structural change in a local window `W` around every pixel position $(u,v)$ in an image. In the literature, the matrix $H$ is often referred to as **covariance matrix**.

To do this, it helps to visualize the matrix $H$ as an ellipse, whose axis length and directions are given by its eigenvalues and eigenvectors. As can be seen in the following figure, the larger eigenvector points into the direction of maximal intensity change, whereas the smaller eigenvector points into the direction of minimal change. So in order to identify corners, we need to find positions in the image which have two significantly large eigenvalues of $H$.

![](https://video.udacity-data.com/topher/2020/September/5f5b839d_9/9.png)

Without going into details on eigenvalues in this course, we will look at a simple formula of how they can be computed from $H$:

![](https://video.udacity-data.com/topher/2020/September/5f5b83c0_10/10.png)

**In addition to the smoothing of the image before gradient computation, the Harris detector uses a Gaussian window `w(x,y)` to compute a weighted sum of the intensity gradients around a local neighborhood.** The size of this neighborhood is called **scale** in the context of feature detection and *it is controlled by the standard deviation of the Gaussian distribution*.

![](https://video.udacity-data.com/topher/2020/September/5f5b83d6_11/11.png)

As can be seen, the larger the scale of the Gaussian window, the larger the feature below that contributes to the sum of gradients. By adjusting scale, we can thus control the keypoints we are able to detect.


### The Harris Corner Detector

Based on the eigenvalues of $H$, one of the most famous corner detectors is the Harris detector. This method evaluates the following expression to derive a corner response measure at every pixel location with the factor `k` being an empirical constant which is usually in the range between `0.04` and `0.06`.

![](https://video.udacity-data.com/topher/2020/September/5f5b83fb_12/12.png)

Based on the concepts presented in this section, the following code computes the corner response for a given image and displays the result.

```c++
// Load image from file.
cv::Mat img{ cv::imread("../images/img1.png") };

// Convert image to grayscale
cv::Mat imgGray; 
cv::cvtColor(img, imgGray, cv::COLOR_BGR2GRAY);

// Detector parameters.
int blockSize{ 2 };     // For every pixel, a blockSize × blockSize neighborhood is considered.
int apertureSize{ 3 };  // aperture parameter for Sobel operator (must be odd)
int minResponse{ 100 }; // minimum value for a corner in the 8bit scaled response matrix
double k{ 0.04 };       // Harris parameter (see equation for details)

// Detect Harris corners and normalize output.
cv::Mat dst, dst_norm, dst_norm_scaled;
dst = cv::Mat::zeros(img.size(), CV_32FC1);
cv::cornerHarris(img, dst, blockSize, apertureSize, k, cv::BORDER_DEFAULT);
cv::normalize(dst, dst_norm, 0, 255, cv::NORM_MINMAX, CV_32FC1, cv::Mat());
cv::convertScaleAbs(dst_norm, dst_norm_scaled);

// Visualize results.
std::string windowName{ "Harris Corner Detector Response Matrix" };
cv::namedWindow(windowName, 4);
cv::imshow(windowName, dst_norm_scaled);
cv::waitKey(0);
```

The result can be seen below : The brighter a pixel, the higher the Harris corner response.

![](https://video.udacity-data.com/topher/2019/April/5cbf9b32_draggedimage-9/draggedimage-9.png)


### Exercise

{% include video id="NKV9IIfxOaI" provider="youtube" %}

In order to locate corners, we now have to perform a **non-maxima suppression** (**NMS**) to:

- ensure that we get the pixel with maximum cornerness in a local neighborhood and
- prevent corners from being too close to each other as we prefer an even spread of corners throughout the image.

Your task is to locate local maxima in the Harris response matrix and perform a **non-maximum suppression** (**NMS**) in a local neighborhood around each maximum. The resulting coordinates shall be stored in a list of keypoints of the type `vector<cv::KeyPoint>`. The result should look like this, with each circle denoting the position of a Harris corner.

![](https://video.udacity-data.com/topher/2019/April/5cbf9b75_draggedimage-10/draggedimage-10.png)

>**Note**  
There are several ways to implement a non-maximum suppression (NMS). The general idea is to iterate over all keypoints and inspect their local neighborhood. If a keypoint has been found that is more suitable (i.e. has a stronger response), then this one should be kept. In order to adjust for the scale of the feature detector, the aperture size of the corner detector should be considered when computing and assessing the overlap between keypoints. **You can view the NMS algorithm's implementation steps on the Addendum page next.**

An alternative method to perform NMS can be found [here](https://en.wikipedia.org/wiki/Harris_Corner_Detector#Non-maximum_suppression)

For the current exercise, you can write your code in the `cornerness_harris.cpp` file in the desktop workspace below. After building, you can run the code using the generated `cornerness_harris` executable.



## 6. Addendum - NMS Algorithm

### Addendum - Solved example for non-maximum suppression (NMS) in computer vision

This code example illustrates the basic principle behind non-maximum suppression. The idea is to reduce the intensities (e.g. corner response) in a local neighborhood in such a way that only the strongest response remains. To achieve this, the following method is used in the `/NMS/src/nms_example.cpp` file below:
(Read the inline comments in the workspace below.)

1. Define the size of the local neighborhood (i.e. a sliding window).
2. Loop over all rows and cols of the input image.
3. Center the sliding window on the current location and search for the maximum response value.
4. If the current pixel is identical to the max. response value, then keep it. Otherwise, set it to zero.

Once all pixels have been processed, only the strongest responses will remain. This method is used all through computer vision, especially in the area of feature detection and tracking. Even though there are many different variants of non-maximum suppression, the basic idea and the principle remain the same.

>**Note**  
The function in the `NMS/helper/det_corners.cpp` file shows how to generate corner response images while the **images** folder contains some example images.



## 7. Overview of Popular Keypoint Detectors

### Why is this section important for you?

One of the disadvantages of the Harris detector is that it does not work well with certain transformations of the image content. These might be rotations or scale (i.e. size) changes or even perspective transformations. The aim of this section is to familiarize yourself with the most important types of transformations and the math behind them. Also, you will find an overview of the currently most popular keypoint detectors, including a code example that shows you how to use them.

{% include video id="fqwXIypb6rQ" provider="youtube" %}


### Invariance to Photometric and Geometric Changes

In the literature (and in the OpenCV library), there is a large number of feature detectors (including the Harris detector), we can choose from. ***Depending on the type of keypoint that shall be detected and based on the properties of the images*, the robustness of the respective detector with regard to both photometric and geometric transformations needs to be considered.**

There are four basic transformation types we need to think about when selecting a suitable keypoint detector:

- Rotation
- Scale change
- Intensity change
- Affine transformation

The following figure shows two images in frame i of a video sequence (a) who have been subjected to several transformations in frame `i + n` (b).

![](https://video.udacity-data.com/topher/2019/April/5cbf9d8a_frame-transformations/frame-transformations.jpg)

For the graffiti sequence, which is one of the standard image sets used in computer vision (also see“Scale and Affine invariant interest point detectors” in IJC V 60(1):63-86, 2004) (), we can observe all of the transformations listed above whereas for the highway sequence, when focussing on the preceding vehicle, there is only a scale change as well as an intensity change between frames `i` and `i+n`.

In the following, the above criteria are used to briefly assess the Harris corner detector.

Rotation R :

![](https://video.udacity-data.com/topher/2020/September/5f5b8741_13/13.png)

Intensity change:

![](https://video.udacity-data.com/topher/2020/September/5f5b875f_14/14.png)

Scale change:

![](https://video.udacity-data.com/topher/2020/September/5f5b8774_15/15.png)

Summarizing, the Harris detector is robust under rotation and additive intensity shifts, but sensitive to scale change, multiplicative intensity shifts (i.e. changes in contrast) and affine transformations. However, if it were possible to modify the Harris detector in a way such that it were able to account for changes of the object scale, e.g. when the preceding vehicle approaches, it might be (despite its age), a suitable detector for our purposes.


### Automatic Scale Selection

In order to detect keypoints at their ideal scale, we must know (or find) their respective dimensions in the image and adapt the size of the Gaussian window `w(x,y)` as introduced earlier in this section. If the keypoint scale is unknown or if keypoints with varying size exist in the image, detection must be performed successively at multiple scale levels.

![](https://video.udacity-data.com/topher/2020/September/5f5b8793_16/16.png)

Based on the increment of the standard deviation between two neighboring levels, the same keypoint might be detected multiple times. This poses the problem of choosing the "correct" scale which best represents the keypoint.

In a landmark paper from 1998, Tony Lindeberg published a method for "Feature detection with automatic scale selection". In this paper, he proposed a function `F(x, y, scale)`, which could be used to select those keypoints that showed a stable maximum of `F` over `scale`. The `scale` for which `F` was maximized was termed the "**characteristic scale**" of the respective keypoint.

The following image shows such a function `F` which has been evaluated for several scale levels and exhibits a clear maximum that can be seen as the characteristic scale of the image content within the circular region.

Details of how to properly design a suitable function `F` are not in the focus of this course however. The major take-away is the knowledge that **a good detector is able to automatically select the characteristic scale of a keypoint based on structural properties of its local neighborhood**. Modern keypoint detectors usually possess this ability and are thus robust under changes of the image scale.

![Scale-space signatures of the trace of two details of a sunflower image](https://video.udacity-data.com/topher/2019/April/5cbf9ecc_scale-space-sunflower/scale-space-sunflower.jpg)
>Scale-space signatures of the trace of two details of a sunflower image  
The signature is computed at the central point in each image. The horizontal axis shows effective scale (the log of the scale parameter), whereas the scaling of the vertical axis is linear in the normalized operator response.
Adapted from the paper titled [Feature Detection with Automatic Scale Selection](https://people.kth.se/~tony/papers/cvap198.pdf), by Tony Lindeberg, 1998.


### Overview of Popular Keypoint Detectors

Keypoint detectors are a very popular research area and thus a large number of powerful algorithms have been developed over the years. Applications of keypoint detection include such things as object recognition and tracking, image matching and panoramic stitching as well as robotic mapping and 3D modeling. In addition to invariance under the transformations mentioned above, detectors can be compared for their detection performance and their processing speed.

The Harris detector along with several other "classics" belongs to a group of traditional detectors, which aim at maximizing detection accuracy. In this group, computational complexity is not a primary concern. The following list shows a number of popular classic detectors :

- 1988 Harris Corner Detector (Harris, Stephens)
- 1996 Good Features to Track (Shi, Tomasi)
- 1999 Scale Invariant Feature Transform (Lowe)
- 2006 Speeded Up Robust Features (Bay, Tuytelaars, Van Gool)

In recent years, a number of faster detectors have been developed which aim at real-time applications on smartphones and other portable devices. The following list shows the most popular detectors belonging to this group:

- 2006 Features from Accelerated Segment Test (FAST) (Rosten, Drummond)
- 2010 Binary Robust Independent Elementary Features (BRIEF) (Calonder, et al.)
- 2011 Oriented FAST and Rotated BRIEF (ORB) (Rublee et al.)
- 2011 Binary Robust Invariant Scalable Keypoints (BRISK) (Leutenegger, Chli, Siegwart)
- 2012 Fast Retina Keypoint (FREAK) (Alahi, Ortiz, Vandergheynst)
- 2012 KAZE (Alcantarilla, Bartoli, Davidson)

In this course, we will be using the Harris detector as well as the Shi-Tomasi detector (which is very similar to Harris) as representatives from the first group of "classic“ detectors. From the second group, we will be leveraging the OpenCV to implement the entire list of detectors.

{% include video id="ybf16ErRqVg" provider="youtube" %}

{% include video id="Hel5bW4bD4w" provider="youtube" %}

>**Note**  
The file explained in the video above is provided in the workspace below with the name as `detect_keypoints.cpp`.


### Exercise

Before we go into details on the above-mentioned detectors in the next section, use the OpenCV library to add the FAST detector in addition to the already implemented Shi-Tomasi detector and compare both algorithms with regard to (a) number of keypoints, (b) distribution of keypoints over the image and (c) processing speed. Describe your observations with a special focus on the preceding vehicle.