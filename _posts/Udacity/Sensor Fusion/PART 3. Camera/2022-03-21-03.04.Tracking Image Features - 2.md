---
title : "03.04 — Tracking Image Features - 2"
category :
    - Sensor Fusion
tag : 
    - C++
    - https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313

toc: true  
toc_sticky: true 
use_math : true
---



## 8. Descriptors

### Why is this section important for you?

In this section, you will be introduced to the concept of descriptors. As stated in the introduction to this lesson, descriptors provide you with distinctive information on the surrounding area of a keypoint. The literature differentiates between gradient-based descriptors and binary descriptors, with the latter being a relatively new addition with the clear advantage of computational speed. The idea of this section is to introduce you to:

- A representative of gradient-based descriptors, which is called **Scale Invariant Feature Transform** (**SIFT**), and
- A representative of binary descriptors called **Binary Robust Invariant Scalable Keypoints** (**BRISK**).

After working through this section you will be able to properly understand the difference between both classes and the concluding exercise will enable you to compare them in practice.

{% include video id="0ddUBGq8Gig" provider="youtube" %}

>**Erratum**  
At timestamp 0:45 in the video above, the acronym HOG stands for "histogram of oriented gradients", instead of "history of oriented gradients".


### Detectors vs Descriptors

Before we go into details on how some of the keypoint detectors discussed in the previous section work, let us take a look at the problem ahead of us. Our task is to find corresponding keypoints in a sequence of images that we can use to compute the TTC to a preceding object, e.g. a vehicle. We, therefore, need a way to robustly assign keypoints to each other based on some measure of similarity. In the literature, a large variety of similarity measures (called **descriptors**) have been proposed and in many cases, authors have published both a new method for keypoint detection as well as a similarity measure which has been optimized for their type of keypoints.

Let us refine our terminology at this point :

- A **keypoint detector** is an algorithm that chooses points from an image based on a local maximum of a function, such as the "cornerness" metric we saw with the Harris detector. Keypoints are sometimes referred to as interest points or salient points.
- A **descriptor** is a vector of values, which describes the image patch around a keypoint. There are various techniques ranging from comparing raw pixel values to much more sophisticated approaches such as histograms of gradient orientations.

Descriptors help us to assign similar keypoints in different images to each other. As shown in the figure below, a set of keypoints in one frame is assigned keypoints in another frame such that the similarity of their respective descriptors is maximized and (ideally) the keypoints represent the same object in the image. In addition to maximizing similarity, a good descriptor should also be able to minimize the number of mismatches, i.e. avoid assigning keypoints to each other that do not correspond to the same object.

![](https://video.udacity-data.com/topher/2019/April/5cbfa254_new-group/new-group.jpg)

Before we go into details on a powerful class of detector/descriptor combinations, let us briefly revisit one of the most famous descriptors of all time - the Scale Invariant Feature Transform. The reason we are doing this is two-fold:

- First, this method is still relevant and being used in a large number of applications.
- And second, we need to lay some foundations so that you will be able to better understand and appreciate the contributions of binary descriptors.



## 9. HOG Descriptors and SIFT

### HOG Descriptors

{% include video id="r0BhmnZpyiI" provider="youtube" %}

In the following, we will take a brief look at the family of descriptors based on Histograms of Oriented Gradients (HOG). The basic idea behind HOG is to describe the structure of an object by the distribution of its intensity gradients in a local neighborhood. To achieve this, an image is divided into cells in which gradients are computed and collected in a histogram. The set of histograms from all cells is then used as a similarity measure to uniquely identify an image patch or object.


### Scale-Invariant Feature Transform (SIFT)

One of the best-known examples of the HOG family is the Scale-Invariant Feature Transform (SIFT), introduced in [1999 by David Lowe](https://home.cis.rit.edu/~cnspci/references/dip/feature_extraction/lowe1999.pdf). The SIFT method includes both a keypoint detector as well as a descriptor and it follows a five-step process, which is briefly outlined in the following.

- First, keypoints are detected in the image using an approach called „Laplacian-Of-Gaussian (LoG)“, which is based on second-degree intensity derivatives. The LoG is applied to various scale levels of the image and tends to detect blobs instead of corners. In addition to a unique scale level, keypoints are also assigned an orientation based on the intensity gradients in a local neighborhood around the keypoint.

- Second, for every keypoint, its surrounding area is transformed by removing the orientation and thus ensuring a canonical orientation. Also, the size of the area is resized to 16 x 16 pixels, providing a normalized patch.

    ![The mountain scenery image material has been taken from the original publication by D. Lowe.](https://video.udacity-data.com/topher/2019/April/5cbfc27d_sift-1/sift-1.jpg)
    >The mountain scenery image material has been taken from the original publication by D. Lowe.

- Third, the orientation and magnitude of each pixel within the normalized patch are computed based on the intensity gradients Ix and Iy.

- Fourth, the normalized patch is divided into a grid of 4 x 4 cells. Within each cell, the orientations of pixels which exceed a threshold of magnitude are collected in a histogram consisting of 8 bins.

    ![](https://video.udacity-data.com/topher/2019/April/5cbfc31a_sift-2/sift-2.jpg)

- Last, the 8-bin histograms of all 16 cells are concatenated into a 128-dimensional vector (the descriptor) which is used to uniquely represent the keypoint.

The SIFT detector / descriptor is able to robustly identify objects even among clutter and under partial occlusion. It is invariant to uniform changes in scale, to rotation, to changes in both brightness and contrast and it is even partially invariant to affine distortions.

The downside of SIFT is its low speed, which prevents it from being used in real-time applications on e.g. smartphones. Other members of the HOG family (such as SURF and GLOH), have been optimized for speed. However, they are still too computationally expensive and should not be used in real-time applications.

>**Update**  
Please note that until March 2020, SIFT (and SURF) were heavily patented and thus could not be freely used in a commercial context. In case you have an older version of the OpenCV installed on your system, , you have to `#include <opencv2/xfeatures2d/nonfree.hpp>` in order to use SIFT. In versions of the OpenCV >= 4.3, SIFT and SURF can be used via `#include <opencv2/xfeatures2d.hpp>`.

A much faster (and free) **alternative to HOG-based methods is the family of binary descriptors**, which provide a fast alternative at only slightly worse accuracy and performance. Let us take a look at those in the next section.

#### External Resource
[Medium article on SIFT algorithm](https://towardsdatascience.com/sift-scale-invariant-feature-transform-c7233dc60f37)
[OpenCV tutorial on SIFT](https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html)



## 10. Binary Descriptors and BRISK

### Binary Descriptors

The problem with HOG-based descriptors is that they are based on computing the intensity gradients, which is a very costly operation. This is the main reason why SIFT is typically not used on devices with limited processing capabilities such as smartphones. Even though there have been some advancements such as the development of the very similar SURF algorithm, which uses the less costly integral image instead of intensity gradients, we do not see many real-time applications.

The central idea of binary descriptors is to rely solely on the intensity information (i.e. the image itself) and to encode the information around a keypoint in a string of binary numbers, which can be compared very efficiently in the matching step when corresponding keypoints are searched. Currently, the most popular binary descriptors are BRIEF, BRISK, ORB, FREAK, and KAZE (all available in the OpenCV library). A comparison of several region detectors can be found in the paper titled [Image matching using SIFT, SURF, BRIEF, and ORB: performance comparison for distorted images](https://arxiv.org/pdf/1710.02726.pdf), by (Karami E. et al., 2017).

From a high-level perspective, binary descriptors consist of three major parts:

- A **sampling pattern** which describes where sample points are located around the location of a keypoint.
- A method for **orientation compensation**, which removes the influence of rotation of the image patch around a keypoint location.
- A method for **sample-pair selection**, which generates pairs of sample points that are compared against each other with regard to their intensity values. If the first value is larger than the second, we write a '1' into the binary string, otherwise we write a '0'. After performing this for all point pairs in the sampling pattern, along the binary chain (or ‘string’) is created (hence the family name of this descriptor class).


### Binary Robust Invariant Scalable Keypoints (BRISK)

In the following, the [Binary Robust Invariant Scalable Keypoints](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/43288/1/eth-7684-01.pdf) (BRISK), the keypoint detector/descriptor is used as a representative for the binary descriptor family. Proposed in 2011 by Stefan Leutenegger et al., BRISK is a FAST-based detector in combination with a binary descriptor created from intensity comparisons retrieved by a dedicated sampling of each keypoint neighborhood.

As stated in the original paper, points of interest are identified across both the image and scale dimensions using a saliency criterion similar to the AGAST method. For more details on how this is works, please refer to the original paper which is listed in the [OpenCV documentation](https://docs.opencv.org/3.4/d0/de3/citelist.html#CITEREF_LCS11).

The sampling pattern of BRISK is composed of a number of sample points (blue), where a concentric ring (red) around each sample point denotes an area where Gaussian smoothing is applied. As opposed to some other binary descriptors such as ORB or BRIEF, the BRISK sampling pattern is fixed. The smoothing is important to avoid aliasing (an effect that causes different signals to become indistinguishable - or aliases of one another - when sampled).

![Binary Robust Invariant Scalable Keypoints](https://video.udacity-data.com/topher/2019/April/5cbfc3b9_brisk-1/brisk-1.png)
>Image source - [Binary Robust Invariant Scalable Keypoints](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/43288/1/eth-7684-01.pdf), by Stefan Leutenegger et al., 2011

During sample pair selection, the BRISK algorithm differentiates between long- and short-distance pairs. The long-distance pairs (i.e. sample points with a minimal distance between each other on the sample pattern) are used for estimating the orientation of the image patch from intensity gradients, whereas the short-distance pairs are used for the intensity comparisons from which the descriptor string is assembled.

![Mathematical description of pairs for BRISK algorithm](https://video.udacity-data.com/topher/2019/May/5cd315cd_draggedimage/draggedimage.png)

First, we define set A of all possible pairings of sample points. Then, we extract the subset L from A for which the euclidean distance is above a pre-defined upper threshold. This set is the long-distance pairs used for orientation estimation. Lastly, we extract those pairs from A whose euclidean distance is below a lower threshold. This set S contains the short-distance pairs for assembling the binary descriptor string.

The following figure shows the two types of distance pairs on the sampling pattern for long pairs (left) and short pairs (right).

![](https://video.udacity-data.com/topher/2019/May/5cd3166e_new-group-1/new-group-1.jpg)
>Image source [here](https://www.cse.unr.edu/~bebis/CS491Y/Lectures/BRISK.pptx), Presentation by Josh Gleason, BRISK, International Conference on Computer Vision, 2011

![](https://video.udacity-data.com/topher/2019/May/5cd31743_draggedimage-1/draggedimage-1.png)

First, the gradient strength between two sample points is computed based on the normalized unit vector that gives the direction between both points multiplied with the intensity difference of both points at their respective scales. Then, the keypoint direction vector \vec{g} is computed from the sum of all gradient strengths.

Based on \vec{g}, we can use the direction of the sample pattern to rearrange the short-distance pairings and thus ensure rotation invariance. Based on the rotation-invariant short-distance pairings, the final binary descriptor can then be constructed

![](https://video.udacity-data.com/topher/2019/May/5cd317b6_draggedimage-2/draggedimage-2.png)

After computing the orientation angle of the keypoint from g, we use it to make the short-distance pairings invariant to rotation. Then, the intensity between all pairs in `S` is compared and used to assemble the binary descriptor we can use for matching.



## 11. HOG vs. Binary Exercise

### HOG vs. Binary Exercise

{% include video id="69oJ9Jx2DPE" provider="youtube" %}

In the code at the end of this section, keypoints and descriptors are computed using the BRISK method. The time for both keypoint detection and descriptor computation is printed to the console. For the BRISK detector, the keypoints can be seen in the following figure with the center of a circle denoting its location and the size of the circle reflecting the characteristic scale.

![](https://video.udacity-data.com/topher/2019/April/5cbfc43e_draggedimage/draggedimage.png)

Given the code in `describe_keypoints.cpp` of the workspace below, add the SIFT detector / descriptor, compute the time for both steps and compare both BRISK and SIFT with regard to processing speed and the number and visual appearance of keypoints.

After building your code with `cmake` and `make`, you can run the code from the virtual desktop using the `describe_keypoints` executable.

In the next section, we will look at the descriptor part of BRISK in detail.


### Recommended Read

Now that you have seen how to integrate BRISK in your implementation, in the same manner, you will have to incorporate a few more algorithms in your upcoming project (which can be done quickly with the OpenCV). You must have noticed that the [FeatureDetector](https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_feature_detectors.html#featuredetector-create) and [DescriptorExtractor](https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_descriptor_extractors.html#descriptorextractor-create) are two separate sets having a few common algorithms, such as BRISK. The implementation of FeatureDetector comes before the DescriptorExtractor.

The additional methods you must learn to integrate are BRIEF, ORB, FREAK, AKAZE, and SIFT. **In practice, you would need to implement and apply all of them to the images relevant to the project you are working on** (e.g. pedestrian detection, aerial images). Therefore, we recommend you go through the [openCV documentation](https://docs.opencv.org/2.4/modules/features2d/doc/feature_detection_and_description.html) for a quick overview. Particularly, read more about the:

- [BRIEF](https://docs.opencv.org/master/dc/d7d/tutorial_py_brief.html) - Note that the BRIEF is a DescriptorExtractor, not a FeatureDetector, which is why you cannot use the BRIEF algorithm in the factory method `FeatureDetector::create()`. You may find [this](https://stackoverflow.com/questions/29870726/brief-implementation-with-opencv-2-4-10) discussion on using BRIEF with OpenCV helpful.
- [ORB](https://docs.opencv.org/3.4/d1/d89/tutorial_py_orb.html) keypoint detector and descriptor extractor
- [AKAZE](https://docs.opencv.org/3.4/db/d70/tutorial_akaze_matching.html) local features matching and view the class reference for AKAZE [here](https://docs.opencv.org/3.4/d8/d30/classcv_1_1AKAZE.html#details).
    >**Note** that the AKAZE descriptor works only with AKAZE keypoints.
- (Optional) [SURF](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html#introduction-to-surf-speeded-up-robust-features)
    - [feature detec tion tutorial](https://docs.opencv.org/3.4/d7/d66/tutorial_feature_detection.html), and
    - [feature description tutorial](https://docs.opencv.org/3.4/d5/dde/tutorial_feature_description.html)

>**Update**  
Please note that until recently, SIFT and SURF were heavily patented, and thus could not be freely used in a commercial context. In case you have an older version of the OpenCV installed on your system, you have to `#include <opencv2/xfeatures2d/nonfree.hpp>` in order to use both algorithms. In versions of the OpenCV >= 4.3, SIFT and SURF can be used via `#include <opencv2/xfeatures2d.hpp>`.



## 12. Exercise - Descriptor Matching

### Why is this section important for you?

Once you have located and described a set of keypoints for each image in a sequence of frames, the next step in the tracking process is to find the best fit for each keypoint in successive frames. In order to do so, you need to implement a suitable similarity measure so that your tracking algorithm can uniquely assign keypoint pairs. This section will give you an overview of the available similarity measures, including their respective advantages and disadvantages.

Also, you will learn how to properly deal with ambiguities that can occur when features look very similar. The section will conclude with an overview of performance measures (e.g. true positive rate) to describe the quality of a matching method so you will be able to compare algorithms and make an informed choice when it comes down to selecting a suitable method for your project.

{% include video id="IGa4AUyl7YQ" provider="youtube" %}


### Distance between descriptors
In the last section, you have learned that keypoints can be described by transforming their local neighborhood into a high-dimensional vector that captures the unique characteristics of the gradient or intensity distribution. In this section, we want to look at several ways to compute the distance between two descriptors such that the differences between them are transformed into a single number which we can use as a simple measure of similarity.

The first distance function is the "Sum of Absolute Differences (SAD)". As you can see in the equation below, the SAD takes as input two descriptor vectors `d_a` and `d_b`. The SAD is computed by subtracting from every component in `d_a` the corresponding component at the same position in `d_b`. Then, the absolute value of the respective result is summed up. The SAD norm is also referred to as L1-norm in the literature.

The second distance function is the "Sum of Squared Differences (SSD)", which is similar to the SAD in the sense that differences between individual components of two descriptor vectors are computed. However, the key difference between SAD and SSD is that the latter sums the squared differences instead of the absolute differences. In the literature, the SSD norm is also referred to as L2-norm. Both norms are given in the following figure.

![](https://video.udacity-data.com/topher/2020/September/5f5b8a78_17/17.png)

There are several ways of explaining the differences between SAD and SSD. One helpful approach, as we want to keep this aspect short, is to look at both norms from a geometrical perspective. In the following figure, a two-dimensional feature space is considered. In it, there are two feature vectors d1 and d2, each of which consists of an (a,b) coordinate pair.

![](https://video.udacity-data.com/topher/2020/September/5f5b8abe_18/18.png)

The shortest distance between both is a straight line. Given the two components of each vector, the SAD computes the sum of the length differences, which is a one-dimensional process. The SSD on the other hand, computes the sum of squares, which obeys the law of Pythagoras. This law says that in a rectangular triangle, the sum of the catheti squares is equal to the square of the hypotenuse. So in terms of the geometric distance between both vectors, the L2-norm is a more accurate measure. Note that the same rationale applies to higher-dimensional descriptors in the same manner.

In the case of a binary descriptor who consists only of ones and zeros, the best (and fastest) measure to use is the Hamming distance, which computes the difference between both vectors by using an XOR function, which returns zero if two bits are identical and one if two bits are different. So the sum of all XOR operations is simply the number of differing bits between both descriptors.

The key takeaway here is that you have to adapt the distance measure to the type of descriptor you are using. In case of gradient-based methods such as SIFT, the L2-norm would be most appropriate. In the case of all binary descriptors, the Hamming distance should be used.


### Finding matches

Let us assume we have N keypoints and their associated descriptors in one image and M keypoints in another image. The most obvious method to look for corresponding pairs would be to compare all features with each other, i.e. perform N x M comparisons. For a given keypoint from the first image, it takes every keypoint in the second image and calculates the distance. The keypoint with the smallest distance will be considered its pair. This approach is referred to as Brute Force Matching or Nearest Neighbor Matching and is available in the OpenCV under the name BFMatcher. The output of brute force matching in OpenCV is a list of keypoint pairs sorted by the distance of their descriptors under the chosen distance function. Brute force matching works well for small keypoint numbers but can be computationally expensive as the number of keypoints increases.

In 2014, David Lowe (the father of SIFT) and Marius Muja released an open-source library called "fast library for approximate nearest neighbors" (FLANN). FLANN trains an indexing structure for walking through potential matching candidates that is created using concepts from machine learning. The library builds a very efficient data structure (a KD-tree) to search for matching pairs and avoids the exhaustive search of the brute force approach. It is therefore faster while the results are still very good, depending on the matching parameters. As FLANN-based matching entails a whole new body of knowledge with several concepts that have limited relevance for this course, there is no detailed description of the method given here. The FLANN-based matching is available in the OpenCV and you will see it again in the code example below. At the time of writing (May 2019), there is a potential bug in the current implementation of the OpenCV, which requires a conversion of the binary descriptors into floating point vectors, which is inefficient. Yet still there is an improvement in speed, albeit not as large as it potentially could be.

Both BFMatching and FLANN accept a descriptor distance threshold T which is used to limit the number of matches to the ‘good’ ones and discard matches where the respective pairs are no correspondences. Corresponding ‘good’ pairs are termed ‘True Positives (TP)’ whereas mismatches are called ‘False Positives (FP)’. The task of selecting a suitable value for T is to allow for as many TP matches as possible while FP matches should be avoided as far as possible. Depending on the image content and on the respective detector / descriptor combination, a trade-off between TP and FP has to be found that reasonably balances the ratio between TP and FP. The following figure shows two distributions of TP and of FP over the SSD to illustrate threshold selection.

![](https://video.udacity-data.com/topher/2020/September/5f5b8aeb_19/19.png)

The first threshold T1 is set to a maximally permissible SSD between two features in a way that some true positive matches are selected, while false positive matches are almost entirely avoided . However, most TP matches are also discarded with this setting. By increasing the matching threshold to T2, more TP matches are selected but the number of FP matches also increases significantly. In practice, a clear and concise separation of TP and FP is almost never found and therefore, setting a matching threshold is always a compromise between balancing 'good' vs. 'bad' matches. While FP matches can not be avoided in most cases, the goal always is to lower their number as much as possible. In the following, two strategies to achieve this are presented.


### Selecting Matches
As long as the selected threshold T is not exceeded, brute force matching will always return a match for a keypoint in the first image, even if the keypoint is not present in the second image. This inevitably leads to a number of false matches. A strategy to counteract this is called cross check matching, which works by applying the matching procedure in both directions and keeping only those matches whose best match in one direction equals the best match in the other direction. The steps of the cross check approach are:

1. For each descriptor in the source image, find one or more best matches in the reference image.
2. Switch the order of source and reference image.
3. Repeat the matching procedure between source and reference image from step 1.
4. Select those keypoint pairs whose descriptors are best matches in both directions.

Although cross check matching increases the processing time, it usually removes a significant number of false matches and should thus always be performed when accuracy is preferred over speed.

A very efficient way of lowering the number of false positives is **to compute the nearest neighbor distance ratio for each keypoint**. This method has been originally proposed by D. Lowe in the 1999 paper on SIFT. The main idea is to not apply the threshold on the SSD directly. Instead, for each keypoint in the source image, the two best matches are located in the reference image and the ratio between the descriptor distances is computed. Then, a threshold is applied to the ratio to sort out ambiguous matches. The figure below illustrates the principle.

![](https://video.udacity-data.com/topher/2020/September/5f5b8c0d_20/20.png)

In the example, an image patch with an associated descriptor da is compared to two other image patches with descriptors `d_{b_1}` and `d_{b_2}`. As can be seen, the patches look very similar and would result in ambiguous and thus unreliable matches. By computing the SSD ratio between best and second-best match, such weak candidates can be filtered out.

In practice, a threshold value of 0.8 has proven to provide a good balance between TP and FP. In the image sequences examined in the original SIFT paper, 90% of the false matches were eliminated with this setting while less than 5% of correct matches were lost.


### Exercise: Distance between descriptors

{% include video id="nawEaRwnu2c" provider="youtube" %}

In the following code example, a set of binary BRISK descriptors is pre-loaded and matched using the brute force approach described earlier in this section. Note that the number of matches has been limited to the 100 best candidates for educational purposes, as it is much easier to visually spot mismatches when drawing a reduced number of keypoint pairs as an overlay. Note that once the matches have been computed, the function can be set to either nearest-neighbor (keeping only the best match) or k-nearest-neighbor selection (keeping the best k matches per keypoint).

Before we take a look at how to estimate the performance of keypoints and descriptors further down below in this section, please complete the following tasks using the desktop workspace below:

1. Load the 'BRISK_small' dataset with cross-check first turned off, then on. Look at the visualized keypoint matches and at the number of matched pairs and describe your results.
2. Add the k-nearest-neighbor matching (using `cv::knnMatch`) with `k = 2` and implement the above-described descriptor distance ratio to filter out ambiguous matches with the threshold set to 0.8. Visualize the results, count the percentage of discarded matches (for both the 'BRISK_small' and the 'BRISK_large' dataset) and describe your observations.
3. Use both BF matching and FLANN matching on the 'BRISK_large' dataset and on the SIFT dataset and describe your observations.

The code for this exercise is in the `descriptor_matching.cpp` file, and after building with `cmake` and `make`, you can run the code using the `descriptor_matching` executable.



## 13. Evaluating Matching Performance

### Evaluating Matching Performance

There exists a large number of detector and descriptor types and based on the problem to be solved, a suitable pair of algorithms has to be chosen based on requirements such as the accuracy of keypoints or the number of matched pairs. In the following, an overview of the most common measures is presented.

- The **True Positive Rate** (**TPR**) is the ratio between keypoints that have been matched correctly (true positives - TP) and the sum of all potential matches, including ones missed by the detector/descriptor (false negatives - FN). A perfect matcher would have a TPR of 1.0 as there would be no false matches. In the literature, the TPR is also referred to as recall and can be used to quantify how many of the possible correct matches were actually found.
- The **False Positive Rate** (**FPR**) is the ratio between keypoints that have been incorrectly matched (false positives - FP) and the sum of all features that should have been without a match. A perfect matcher would have a FPR of 0.0. The FPR is also referred to as false alarm rate and describes how likely it is that a detector / descriptor selects a wrong keypoint pair.
- The **Precision** of a matcher is the number of correctly matched keypoints (TP) divided by the number of all matches. This measure is also referred to as inlier ratio.

The following table gives an overview of some of the measures just introduced.

![](https://video.udacity-data.com/topher/2019/May/5cd320ed_draggedimage-4/draggedimage-4.png)

In order to decide whether a match or non-match is correct, ground-truth information of the image material used for assessment is needed. In many publications, image sequences have been used where all keypoints are located on a planar surface. In such a case, a model-based estimation can be used to differentiate between TP / TN and FP / FN. For the image sequences we use in this course, this approach can not be used as "our" keypoints are distributed in a complex three-dimensional scene, in which the objects move dynamically with unknown motion parameters. However, we can use a large number of available comparisons in the literature and transfer the results to our application scenario. At the end of this section, a small selection of such results is shown.

- The **Receiver Operating Characteristic** (**ROC**) is a graphical plot that shows how well a detector / descriptor is able to differentiate between true and false matches as its discrimination threshold is varied. The ROC can be used to visually compare different detectors / descriptors and also select a suitable discrimination threshold for each. The name ROC dates back to WW II, where the method has been introduced by radar operators in the context of identifying enemy targets.

- The following figure shows how the ROC is constructed from the distribution of true positives and false positives by varying the discrimination threshold on the SSD. An ideal detector / descriptor would have a TPR of 1.0 while the FPR would be close to 0.0 at the same time.
    ![](https://video.udacity-data.com/topher/2020/September/5f5b8b91_21/21.png)

- In the following figure, two examples for good and bad detectors / descriptors are shown. In the first example, there is no way of safely differentiating between TP and FP as both curves match and changes in the discrimination threshold would affect them in the same way. In the second example, the TP and FP curve do not overlap significantly and therefore, a suitable discriminator threshold can be selected.
    ![](https://video.udacity-data.com/topher/2020/September/5f5b8bbe_22/22.png)

There are several other ways of assessing detectors and descriptors such as the **Precision-Recall Curve** which we will not discuss in this course in order to stay focused on the task ahead - **which is the development of our collision avoidance system**.


### Conclusion

To conclude this section, an overview of results is given, where several descriptors have been compared against each other and which can be used to facilitate the selection process when detectors / descriptors have to be chosen for an application. In the graph, you can see the ROC curves of different descriptors such as SIFT, BRISK and several others and visually compare them against each other.

>**Note**  
Those results are only valid for the image sequences actually used for the comparison - for a different image set, e.g. a traffic scene, results may vary significantly.

![Efficient discriminative projections for compact binary descriptors](https://video.udacity-data.com/topher/2019/May/5cd321cb_results1/results1.png)
>Image source - Research paper titled [Efficient discriminative projections for compact binary descriptors](https://link.springer.com/content/pdf/10.1007/978-3-642-33718-5_17.pdf), in European Conference on Computer Vision, by Trzcinski et. al., 2012


#### Want to compare on your own?

You can compare any two or more detectors/descriptors based on the performance metric(s) of your choice. Here are a few publicly available datasets:
- [Oxford Dataset](https://www.robots.ox.ac.uk/~vgg/data/)
- [HPatches: Homography-patches dataset](https://github.com/hpatches/hpatches-dataset)

One can measure these algorithms' performance in terms of the performance metrics discussed above. Other relevant metrics could be - keypoint verification, keypoint retrieval, number of matching keypoints, efficiency, speed, accuracy of keypoints (precision and recall), repeatability, correspondences, efficiency, duration, or the average distance.

**In general, you will notice that the ORB and BRISK generally perform better than other binary descriptors. Whereas in the gradient-based descriptors, SIFT is much better than SURF. Though the results are dependent on the underlying dataset, and the environment you are working in.**


#### References

Here are a few good articles that have compared various feature detectors:

- Hidalgo, F., et. al. (2020). [Evaluation of Several Feature Detectors/Extractors on Underwater Images towards vSLAM](https://www.mdpi.com/1424-8220/20/15/4343/pdf). Sensors, 20(15), 4343.
- Tareen, S. A. K., et. al. (2018). [A comparative analysis of sift, surf, kaze, akaze, orb, and brisk](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8346440). In 2018 International conference on computing, mathematics and engineering technologies (iCoMET) (pp. 1-10). IEEE.
- Işık, Ş. (2014). [A comparative evaluation of well-known feature detectors and descriptors](https://dergipark.org.tr/en/download/article-file/89429). International Journal of Applied Mathematics Electronics and Computers, 3(1), 1-6.
- Bojanić, D., et. al. (2019). [On the comparison of classic and deep keypoint detector and descriptor methods](https://arxiv.org/pdf/2007.10000.pdf). In 2019 11th International Symposium on Image and Signal Processing and Analysis (ISPA) (pp. 64-69). IEEE.
- [A Comparison of SIFT, SURF and ORB](https://medium.com/@shehan.a.perera/a-comparison-of-sift-surf-and-orb-333d64bcaaea) Medium article



### Lesson Summary

{% include video id="vosXnbzD8V8" provider="youtube" %}

**External resource:** [OpenCV tutorial - 2D Features framework (feature2d module)](https://docs.opencv.org/master/d9/d97/tutorial_table_of_content_features2d.html)











## 14. Addendum - ORB

You must have noticed that the [Detector](https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_feature_detectors.html#featuredetector-create) and [Descriptor](https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_descriptor_extractors.html#descriptorextractor-create) are two separate sets having a few common algorithms, such as BRISK and ORB. The implementation of Detector comes before the Descriptor.

Now that you have seen how to integrate BRISK in your implementation, in the same manner, you will have to incorporate a few more algorithms in your upcoming project (which can be done quickly with the [OpenCV](https://docs.opencv.org/2.4/modules/features2d/doc/feature_detection_and_description.html)). The additional methods you must learn to integrate are BRIEF, ORB, FREAK, AKAZE, and SIFT.

>In practice, you would need to implement and apply all of them to the images relevant to the project you are working on (e.g. pedestrian detection, aerial images).

Before you implement any of them in the mid-term project next, it will be a good idea to have a basic understanding of the underlying concepts. Below, we have explained the fundamentals of another best-performing algorithm, ORB.


### Oriented FAST and Rotated BRIEF (ORB)

[ORB](https://docs.opencv.org/3.4/d1/d89/tutorial_py_orb.html) is a combination of a keypoint detector and descriptor algorithms. **It works in two steps:**

1. **Keypoint detection using FAST** - ORB starts by finding keypoints. In general, you can think of a keypoint as a small region in an image that's particularly distinctive. For example, corners in an image, where the pixel values sharply change from light to dark. Once the key points in the image had been located, ORB then calculates a corresponding feature vector for each keypoint in the image. The ORB algorithm creates feature vectors that only contain ones and zeros. For this reason, they're called the binary feature vectors.

    ![Keypoints around the cat's eyes and at the edges of its facial features](https://video.udacity-data.com/topher/2021/March/605485d6_screenshot-2021-03-19-at-4.34.58-pm/screenshot-2021-03-19-at-4.34.58-pm.png)  
    Keypoints around the cat's eyes and at the edges of its facial features.

2. **Description using BRIEF** - ORB uses BRIEF, which in turn identifies the objects in images using the feature vectors created earlier. The sequence of ones and zeros in the feature vectors vary according to a specific keypoint and its surrounding pixel area. The vector represents the patterns of intensity around a key point. So, multiple feature vectors can be used to identify a larger area and even a specific object in an image.

ORB, is not only incredibly fast, but it's also impervious to noise illumination, and image transformations such as rotations. As you know that ORB stands for Oriented fast and Rotated Brief. Next, we'll take a closer look at how the fast and brief algorithms work.


#### Features from Accelerated Segments Test (FAST)

{% include video id="DCHAc6fjcVM" provider="youtube" %}

![Zoomed in patch around an arch in a building](https://video.udacity-data.com/topher/2018/April/5ade7fed_screen-shot-2018-04-23-at-5.52.20-pm/screen-shot-2018-04-23-at-5.52.20-pm.png)
>Zoomed in patch around an arch in a building. Original image taken from OpenCV's documentation.

#### QUIZ QUESTION

- **Q.** Do you think the pixel, p, above, will be identified as a keypoint by the FAST algorithm?

- **A.** Yes. There are enough connecting pixels around p that are brighter than p, and so it qualifies as a keypoint.


#### Binary Robust Independent Elementary Features (BRIEF)

{% include video id="EKIPEPpRciw" provider="youtube" %}

{% include video id="2k3T6rfjvx0" provider="youtube" %}

>**Note**
BRIEF is a DescriptorExtractor, not a FeatureDetector, which is why you cannot use the BRIEF algorithm in the factory method FeatureDetector::create(). You may find this discussion on using BRIEF with OpenCV helpful.


### Other Algorithms of Interest

- [AKAZE](https://docs.opencv.org/3.4/db/d70/tutorial_akaze_matching.html) local features matching and view the class reference for AKAZE [here](https://docs.opencv.org/3.4/d8/d30/classcv_1_1AKAZE.html#details). **Note that the AKAZE descriptor works only with AKAZE keypoints.**

- (Optional) [SURF](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html#introduction-to-surf-speeded-up-robust-features) / [SURF: Speeded Up Robust Features, by Bay et. al., 2006](https://link.springer.com/content/pdf/10.1007/11744023_32.pdf)



## 15. Tracking an Object Across Images

{% include video id="xuW1xaLYOng" provider="youtube" %}