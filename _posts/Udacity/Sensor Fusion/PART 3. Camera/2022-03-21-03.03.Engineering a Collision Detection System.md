---
title : "03.03 — Engineering a Collision Detection System"
category :
    - Sensor Fusion
tag : 
    - C++
    - https://www.udacity.com/course/sensor-fusion-engineer-nanodegree--nd313

toc: true  
toc_sticky: true 
use_math : true
---



## 1. Collision Detection Basics

### Overview

In this lesson, you will learn the basics of how to design and model a collision detection system. First, you will be introduced to the motion equations and models that govern an accelerating or decelerating vehicle's behavior. Then, I will show you how you can estimate the time-to-collision (TTC) to a vehicle using Lidar measurements.

After understanding how to compute the TTC based on Lidar distance measurement, you will then learn how to use a camera for TTC computation based on information in two-dimensional images.

Lastly, I will give you an overview of the collision detection system and its structure you will be successively building in this course both in the mid-term and the final project.

{% include video id="td8UlKUk5wc" provider="youtube" %}


#### Why is this section important for you?

This section will help you understand the nature of the problem you need to solve: reconstruct the motion of a vehicle in front of the sensor based on time, distance, velocity, and acceleration. We will look at two motion models that rely on different assumptions, such as the velocity or the acceleration of the preceding vehicle are constant. Such assumptions make the problem of motion reconstruction easier to solve but also less accurate if they are violated in a real-life scenario. The main idea of this section is to familiarize yourself with the basic assumptions behind the models as well as the equations so you can implement them in code in one of the next sections.


### The Collision Detection Problem

A collision avoidance system (CAS) is an active safety feature that warns drivers or even triggers the brake in the event of an imminent collision with an object in the path of driving. If a preceding vehicle is present, the CAS continuously estimates the time-to-collision (TTC). When the TTC falls below a lower threshold, the CAS can then decide to either warn the driver of the imminent danger or - depending on the system - apply the vehicle brakes autonomously. For the engineering task you will be completing in this course this means that you will need to find a way to compute the TTC to the vehicle in front.

Let us take a look at the following scene:

![Traffic Scenario](https://video.udacity-data.com/topher/2020/September/5f5a264f_1/1.png)
>Traffic Scenario

In this traffic scenario, the green vehicle starts to reduce its speed at time `t_0`, which is when the yellow vehicle, equipped with a collision sensor, takes the distance measurement `d_0`. A moment later, at time `t_1`, the green vehicle is considerably closer and a second measurement `d_1` is taken. The goal now is to compute the remaining TTC so the system can warn the driver of the yellow vehicle or even trigger the brakes autonomously.

Before we can do this however, we need to find a way to describe the relative motion of the vehicles with a mathematical model. Note that all the velocities mentioned in the models below are relative velocities between the vehicle that is carrying the sensor and the preceding vehicle which is scanned by the sensor.


### Constant velocity vs. constant acceleration

To compute the TTC, we need to make assumptions on the physical behavior of the preceding vehicle. One assumption could be that the relative velocity between the yellow and green vehicle in the above figure were constant. This would lead to the so-called constant velocity model (CVM) which is represented by eq. 1 in the following diagram. Note that vv stands for velocity and dd stands for distance, which should not be confused with a derivative operator.

![](https://video.udacity-data.com/topher/2020/September/5f5a266d_2/2.png)

As you can see, the distance to the vehicle at time instant t + \Delta tt+Δt is smaller than at time tt, because we subtract the product of a constant relative velocity `v_0` and time \Delta tΔt. From an engineering perspective, we would need a sensor capable of measuring the distance to the preceding vehicle on a precisely times basis with a constant dtdt between measurements. This could very well be achieved with e.g. a Lidar sensor.

Especially in dynamic traffic situations where a vehicle is braking hard, the CVM is not accurate enough, however, as the relative velocity between both vehicles changes between measurements. In the following figure, the approaching vehicle is shown at three time instants with increasing velocity.

![](https://video.udacity-data.com/topher/2020/September/5f5a26a2_3/3.png)

We can thus expand on our CVM by assuming velocity to be a function of time and subtract a second term in eq. 2 which is the product of a constant acceleration and the squared time dt between both measurements. Eq. 3 displays velocity as a function of time, which is also dependent on the constant acceleration we used in eq. 2. This model is referred to as constant acceleration model (CAM) and it is commonly used in commercially available collision detection systems. On a side note, if we were using a radar sensor instead of a Lidar, a direct measurement on velocity could be taken by exploiting a frequency shift in the returning electromagnetic wave due to the Doppler effect. This is a significant advantage over sensors such as Lidar, where velocity can only be computed based on (noisy) distance measurements.

In this course, we will be using a CVM instead of the CAM as it is much simpler to handle with regard to the math involved and with regard to the complexity of the programming task ahead of you. For small instances of dtdt, we will assume that the CVM model is accurate enough and that it will give us a decent estimate of the TTC. Should you be involved in building a commercial version of such a system at a later stage in your career, however, keep in mind that you should be using a constant acceleration model instead.

As a conclusion, we have the following types of motion models possible:

- Constant velocity model (CVM) - This one we will consider to be working on.
- Constant acceleration model (CAM) - An ideal case, but still complex as compared to the constant velocity motion model
- Changing acceleration - Real-life scenarios, most often too complex to handle in practice

{% include video id="T8tSjKZaJfQ" provider="youtube" %}


### Exercise

Imagine the following scenario: A preceding vehicle with a relative distance of 25m and a relative speed to the CAS-equipped vehicle of 30km/h is braking hard. The road surface is slippery and the resulting constant deceleration is at 5m/s^2.


### Exercise Solutions

#### Supporting Materials

- [link](https://video.udacity-data.com/topher/2019/May/5ceddbb5_exercise-solutions/exercise-solutions.jpg)



## 2. Estimating TTC with Lidar

### Why is this section important for you?

This section will show you how to estimate the time to collision (TTC) with a preceding vehicle based on Lidar distance measurements. In order to do this, you first need to understand how the TTC can be computed from successive sensor measurements and from the motion model you saw in the last section. Once this is clear, we will look at how to extract a stable distance measurement from the Lidar point cloud by removing irrelevant measurements, e.g. from the road surface. Once this is clear, we will jump right into a piece of code that computes a TTC estimate based on Lidar point clouds.

{% include video id="961l-IJpA6w" provider="youtube" %}


### The Math Behind Time-to-Collision (TTC)

In the following, let us assume that our CAS-equipped vehicle is using a Lidar sensor to take distance measurements on preceding vehicles. The sensor in this scenario will give us the distance to the closest 3D point in the path of driving. In the figure below, the closest point is indicated by a red line emanating from a Lidar sensor on top of the CAS vehicle.

![](https://video.udacity-data.com/topher/2020/September/5f5a2c74_4/4.png)

Based on the model of a constant-velocity we discussed in the last section, the velocity `v_0` can be computed from two successive Lidar measurements as follows:

![](https://video.udacity-data.com/topher/2020/September/5f5a2c94_5/5.png)

Once the relative velocity `v_0` is known, the time to collision can easily be computed by dividing the remaining distance between both vehicles by `v_0`. So given a Lidar sensor which is able to take precise distance measurements, a system for TTC estimation can be developed based based on a CVM and on the set of equations shown above. Note however that a radar sensor would be the superior solution for TTC computation as it can directly measure the relative speed, whereas with the Lidar sensor we need to compute `v_0` from two (noisy) distance measurements.


### Preparing the Lidar Point Cloud

The following image shows a Lidar point cloud as an overlay over a camera image taken in a highway scenario with a preceding vehicle directly in the path of driving. Distance to the sensor is color-coded (green is far away, red is close). On the left side, a bird-view perspective of the Lidar points is shown as well.

![](https://video.udacity-data.com/topher/2019/April/5cbf5aea_ebene/ebene.jpg)

As can easily be seen, the Lidar sensor provides measurements on the vehicles as well as on the road surface. Also, some 3D points in the camera image do not seem accurate when compared to their surrounding neighbors. Especially the points near the roof of the preceding vehicle differ in color from the points on the tailgate.

As measurement accuracy is correlated to the amount of light reflected from an object, it makes sense to consider the reflectiveness r of each Lidar point which we can access in addition to the x, y and z coordinates. The image below highlights high reflectiveness with green, whereas regions with low reflectiveness are shown as red. An analysis of the associated reflectivity of the point cloud shows that such deviations often occur in regions with reduced reflectiveness.

![](https://video.udacity-data.com/topher/2019/April/5cbf5b3e_draggedimage-2/draggedimage-2.png)

In order to derive a stable TTC measurement from the given point cloud, two main steps have to be performed:

1. Remove measurements on the road surface
2. Remove measurements with low reflectivity

In the figure below, Lidar points are shown in a top-view perspective and as an image overlay after applying the filtering. After removing Lidar points in this manner, it is now much easier to derive the distance d(t) to the preceding vehicle.

![](https://video.udacity-data.com/topher/2019/April/5cbf5b9e_pfeile/pfeile.jpg)

In a later lesson, you will learn how to project Lidar points into the camera image and how to perform the removal procedure as seen in the above examples. For now, let us assume that for each time step dt, the Lidar sensor would return the distance d(t+dt) to the preceding vehicle.


### Computing TTC from Distance Measurements

In the code examples in this course, Lidar points are packaged into a data structure called LidarPoints. As seen below, the structure consists of the point coordinates x (forward), y (left) an z (up) in metric coordinates and of the point reflectivity r on a scale between 0 and 1 (high reflectivity).

```c++
// single lidar point in space
struct LidarPoint { 
    double x, y, z; // point position in m
    double r;       // point reflectivity in the range 0-1
};
```

In order to compute the TTC, we need to find the distance to the closest Lidar point in the path of driving. In the figure below, Lidar measurements located on the tailgate of the preceding vehicle are measured at times `t_0`(green) and `t_1`(red). It can be seen, that the distance to the vehicle has decreased slightly between both time instants.

![](https://video.udacity-data.com/topher/2019/April/5cbf5c22_new-group/new-group.jpg)

The following code searches for the closest point in the point cloud associated with `t_0`(`lidarPointsPrev`) and in the point cloud associated with `t_1`(`lidarPointsCurr`). After finding the distance to the closest points respectively, the TTC is computed based on the formula we derived at the beginning of this section.

```c++
void computeTTCLidar(
    std::vector<LidarPoint>& lidarPointsPrev,
    std::vector<LidarPoint>& lidarPointsCurr, 
    double& TTC) {

    // auxiliary variables
    double dT{ 0.1 } // time between two measurements in seconds

    // Find closest distance to Lidar points 
    double minXPrev{ 1e9 }, minXCurr{ 1e9 };
    for (auto it{ lidarPointsPrev.begin() }; it != lidarPointsPrev.end(); ++it)
        minXPrev = minXPrev > it->x ? it->x : minXPrev;

    for (auto it{ lidarPointsCurr.begin() }; it != lidarPointsCurr.end(); ++it)
        minXCurr = minXCurr > it->x ? it->x : minXCurr;

    // Compute TTC from both measurements.
    TTC = minXCurr * dT / (minXPrev - minXCurr);
}
```

Even though Lidar is a reliable sensor, erroneous measurements may still occur. As seen in the figure above, a small number of points is located behind the tailgate, seemingly without connection to the vehicle. When searching for the closest points, such measurements will pose a problem as the estimated distance will be too small. There are ways to avoid such errors by post-processing the point cloud, but there will be no guarantee that such problems will never occur in practice. It is thus a good idea to perform a more robust computation of minXCurr and minXPrev which is able to cope with a certain number of outliers (in the final project, you will do this) and also look at a second sensor which is able to compute the TTC, such as the camera.


### Exercise

{% include video id="YWatEESYasw" provider="youtube" %}

In the workspace below, extend the function computeTTCLidar shown above so that only Lidar points within a narrow corridor whose width is defined by a variable laneWidth are considered during minimum search. The width of the corridor should be set to 4 meters.

You can run your code as usual by creating a build directory in TTC_lidar. Then use the following steps from within the build directory:

1. `cmake ..`
2. `make`
3. `./compute_ttc_lidar`



## 3. Estimating TTC with a Camera

### Why is this section important for you?

You have already seen how the time to collision (TTC) can be computed from successive distance measurements using a 3D Lidar sensor. Even though it might be somewhat challenging to properly filter the point cloud in some cases, the principle is straightforward. With a 2D camera however, the process of TTC computation is more complicated: First, the camera does not perform 3D measurements but only captures a 2D image of the scene. Second, we need to reliably and accurately identify vehicles in order to track their motion over time.

This section will therefore first show you how to compute the TTC based on the scale change of a vehicle (i.e. its increase in size on the image sensor in successive frames). Then, we will look at an example where a neural network is used to place bounding boxes around vehicles. And then, as bounding boxes are not accurate enough for TTC computation, we will discuss the role of keypoint detection (i.e. distinctive texture patterns) and how clusters of keypoints can be used to accurately and reliably compute a TTC estimate. This section provides you with an overview of the tasks that need to be solved in the final project and more details about those tasks will be added over the next sections.

{% include video id="h8WoErQle2U" provider="youtube" %}


### Measuring TTC without distance
Monocular cameras are not able to measure metric distances. They are passive sensors that rely on the ambient light which reflects off of objects into the camera lens. It is thus not possible to measure the runtime of light as with Lidar technology.

To measure distance, a second camera would be needed. Given two images taken by two carefully aligned cameras (also called a stereo setup) at the same time instant, one would have to locate common points of interest in both images (e.g. the tail lights of the preceding vehicle) and then triangulate their distance using camera geometry and perspective projection. For many years, automotive researchers have developed stereo cameras for the use in ADAS products and some of those have made it to market. Especially Mercedes-Benz has pioneered this technology and extensive information can be found [here](http://www.6d-vision.com/). With more advanced ADAS products and with autonomous vehicles however, stereo cameras have started to disappear from the market due to their package size, the high price and the high computational load for finding corresponding features.

Despite those limitations of the mono camera, let us see if there is a way to compute TTC without the need to measure distance. Let us consider the constant velocity motion model we introduced in a previous section of this course and think about a way to replace the metric distances d with something the camera can measure reliably, such as pixel distances directly on the image plane. In the following figure, you can see how the height HH of the preceding vehicle can be mapped onto the image plane using perspective projection. We can see that the same height HH maps to different heights `h_0` and `h_1` in the image plane, depending on the distance `d_0` and `d_1` of the vehicle. It is obvious that there is a geometric relation between hh, HH, dd and the focal length ff of the pinhole camera - and this is what we want to exploit in the following.

![](https://video.udacity-data.com/topher/2020/September/5f5a2fbb_6/6.png)

Let us take a look at the following set of equations:

![](https://video.udacity-data.com/topher/2020/September/5f5a3220_8/8.png)

In (1) we use the focal length of the camera and a distance measurement `d_0` performed at time `t_0` to project the height H of the vehicle onto the image plane and thus to a height `h_0` in pixels. The same is done at time `t_1`, leading to a projected height `h_1`.

In (2), we compute the ratio of the relative heights `h_0` and `h_1`. As both HH and ff are cancelled out, we can observe a direct relation between relative height hh and absolute metric distance dd. We can thus express the distance to the vehicle `d_0` as the product of `d_1` and the ratio of relative heights on the image plane.

In (3), we substitute `d_0` in the equation for constant velocity and solve for `d_1`, which is now dependent on the constant relative velocity `v_0`, on the time between measuring `d_0` and `d_1` and on the ratio of relative heights on the image plane.

In (4), the TTC is computed as the ratio of remaining distance to impact, which is `d_1`, and the constant velocity `v_0`. As we can easily see, the TTC now only consists of `Δt`, `h_0` and `h_1`.

Thus, it is possible to measure the time to collision by observing relative height change on the image sensor. Distance measurements are not needed and we can thus use a mono camera to estimate the time-to-collision by observing changes in relative height (also called scale change) directly in the image.


### The Problem with Bounding Box Detection

In the figure below, a neural network has been used to locate vehicles in successive images of a monocular camera. For each vehicle, the network returns a bounding box, whose width and/or height could in principle be used to compute the height ratio in the TTC equation we derived in the last section.

When observed closely however, it can be seen that the bounding boxes do not always reflect the true vehicle dimensions and the aspect ratio differs between images. Using bounding box height or width for TTC computation would thus lead to significant estimation errors.

![](https://video.udacity-data.com/topher/2019/April/5cbf7c48_new-group/new-group.jpg)

In most engineering tasks, relying on a single measurement or property is not reliable enough. This holds especially true for safety-related products. Therefore, we want to consider whether there are further properties of vehicles and objects we can observe in an image.


### Using Texture Keypoints Instead

Instead of relying on the detection of the vehicle as a whole we now want to analyze its structure on a smaller scale. If it were possible to locate uniquely identifiable keypoints that could be tracked from one frame to the next, we could use the distance between all keypoints on the vehicle relative to each other to compute a robust estimate of the height ratio in out TTC equation. The following figure illustrates the concept.

![](https://video.udacity-data.com/topher/2020/September/5f5a33c2_9/9.png)

In (a), a set of keypoints has been detected and the relative distances between keypoints 1-7 have been computed. In (b), 4 keypoints have been matched between successive images (with keypoint 3 being a mismatch) using a higher-dimensional similarity measure called descriptor (more about that in the next lesson). The ratio of all relative distances between each other can be used to compute a reliable TTC estimate by replacing the height ratio `h_1 / h_0` with the mean or median of all distance ratios `d_k / (d_k)'`.

The following figure shows several examples of relative distances between keypoints as an overlay over a highway driving scene (only the preceding vehicle is highlighted).

![](https://video.udacity-data.com/topher/2019/April/5cbf8ddb_draggedimage-2/draggedimage-2.png)


### Computing TTC from Relative Keypoint Distances

#### Intro to data structures used in the code

In the code examples in this course, matching keypoints between images are packaged into an OpenCV data structure called cv::DMatch. In order to retrieve the keypoints associated with a match both in the current frame and in the previous frame, we need to make use of the attributes queryIdxand trainIdx, which you will find in the example code below. For more information on the DMatch data structure and its attributes, please refer to [the OpenCV documentation](https://docs.opencv.org/4.4.0/d4/de0/classcv_1_1DMatch.html).

All matched keypoints are stored in a dynamic list, which is then passed to a function called computeTTCCamera, which returns the time-to-collision for each object in the scene. Let us take a look at this function in the following exercise.


### Exercise

{% include video id="2eeP2SWrdiA" provider="youtube" %}

Imagine a set of associated keypoint between two successive frames which contain a large number of mismatches. Computing the mean distance ratio as in the function we just discussed would presumably lead to a faulty calculation of the TTC. A more robust way of computing the average of a dataset with outliers is to use the median instead. In the code below, replace meanDistRatio with a variable medianDistRatio and do not forget to consider both an even and an odd number of values in the vector distRatios.

Additionally, the exercise below uses a term POD, that stands for plain old data. You can refer here for an overview of POD in C++.



## 4. Building Blocks of TTC Computation

### Why is this section important for you?

In the video, you will be given an overview of the various steps and building blocks that are required to compute the time to collision (TTC) based on a Lidar sensor, a camera sensor and on a combination of both. The video will also provide you with an outline of the remaining course and serve as a blueprint for the code you will have to write both in the mid-term and in the final project.

{% include video id="8ViLZSG-ofg" provider="youtube" %}

- **Note** - The lesson numbering shown in the video above (“Lesson 3 - Keypoint Detection and Matching” and “Lesson 4 - Lidar Point Processing and Deep Learning”) does not correspond to the lesson numbering in the current version of the course, as additional content has been added.

#### Good to Read

Research paper titled [No Blind Spots: Full-Surround Multi-Object Tracking for Autonomous Vehicles using Cameras & LiDARs](https://arxiv.org/pdf/1802.08755.pdf) by Rangesh et.al., 2019.



## 5. Early Fusion vs. Late Fusion

{% include video id="enKofyXO1Vk" provider="youtube" %}